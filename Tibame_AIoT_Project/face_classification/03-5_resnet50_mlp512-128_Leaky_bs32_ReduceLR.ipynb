{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1603555428171,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "MMWSB22LC-Ba"
   },
   "outputs": [],
   "source": [
    "# 用單個模型同時執行兩個分類任務:\n",
    "#   age 分成8個classes\n",
    "#   gender 分成2個classes\n",
    "# mlp 每個全連接層的unit個數: 512 - 128 -- 8\n",
    "#                   \n",
    "# trainning: \n",
    "#   改用generator產生資料給fit_generator\n",
    "#   class_weight\n",
    "#   random_state\n",
    "#   callback: EarlyStop, model.save\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "DROP_RATE = 0.4\n",
    "\n",
    "#model_folder_path = 'drive/My Drive/Tibame_AIoT_Project/face'\n",
    "model_folder_path = 'drive/My Drive/Tibame_AIoT_Project'\n",
    "CLEAN_AUG_DATA = 0\n",
    "if CLEAN_AUG_DATA == 1:\n",
    "    img_folder_path = 'drive/My Drive/Tibame_AIoT_Project/Datasets/cleandataset'\n",
    "else:\n",
    "    img_folder_path = 'drive/My Drive/Tibame_AIoT_Project/Datasets/資料集_IMDB-Wiki'\n",
    "    \n",
    "FULL_DATA = 1 #用全部資料\n",
    "#FULL_DATA = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 32753,
     "status": "ok",
     "timestamp": 1603555458821,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "-bv3RgQfbZ_r",
    "outputId": "612fa222-42f6-4308-ace4-d876eee53556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 35499,
     "status": "ok",
     "timestamp": 1603555461576,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Ri1ni0ZmaYPP",
    "outputId": "37c738af-0858-4926-85c4-e97413b7d405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-autotime\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/58/a4a65efcce5c81a67b6893ade862736de355a3a718af5533d30c991831ce/ipython_autotime-0.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from ipython-autotime) (5.5.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.3.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->ipython-autotime) (50.3.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.6.0)\n",
      "Installing collected packages: ipython-autotime\n",
      "Successfully installed ipython-autotime-0.2.0\n",
      "time: 158 µs\n"
     ]
    }
   ],
   "source": [
    "# to measure execution time\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 35493,
     "status": "ok",
     "timestamp": 1603555461580,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "WIxoBByJgDOB",
    "outputId": "077c0a16-680e-42a9-80ea-96550d1bea0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 24 16:04:23 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "time: 112 ms\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "executionInfo": {
     "elapsed": 38164,
     "status": "ok",
     "timestamp": 1603555464258,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Enr3u7SZ0rHX",
    "outputId": "35a1d602-34e1-4ce5-84f2-8ca66da84aeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mtcnn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
      "\r",
      "\u001b[K     |▏                               | 10kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |▎                               | 20kB 27.0MB/s eta 0:00:01\r",
      "\u001b[K     |▍                               | 30kB 19.9MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 40kB 16.8MB/s eta 0:00:01\r",
      "\u001b[K     |▊                               | 51kB 13.7MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 61kB 12.4MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 71kB 12.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 81kB 11.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▎                              | 92kB 11.2MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 102kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 112kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▊                              | 122kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 133kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 143kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 153kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▎                             | 163kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 174kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▋                             | 184kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 194kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 204kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 215kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▏                            | 225kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 235kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 245kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 256kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 266kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 276kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 286kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▏                           | 296kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 307kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 317kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 327kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 337kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 348kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 358kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 368kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 378kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 389kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 399kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 409kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 419kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 430kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 440kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 450kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 460kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 471kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 481kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 491kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 501kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 512kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 522kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 532kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 542kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 552kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 563kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 573kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 583kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 593kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 604kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 614kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 624kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 634kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 645kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 655kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 665kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▋                      | 675kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 686kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 696kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 706kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 716kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 727kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 737kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 747kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 757kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 768kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 778kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 788kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 798kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▌                    | 808kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 819kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 829kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▉                    | 839kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 849kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 860kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 870kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 880kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 890kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 901kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 911kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 921kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 931kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 942kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 952kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 962kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 972kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 983kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 993kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 1.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 1.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▌                 | 1.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 1.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 1.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▏                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▍                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▎               | 1.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▎              | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 1.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▍             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 1.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▏            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████            | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 1.4MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.5MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▊         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████         | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 1.6MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▋        | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▋       | 1.7MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 1.8MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.9MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 2.0MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 2.1MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▍ | 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 2.2MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 2.3MB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 2.3MB 11.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (4.1.2.30)\n",
      "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from mtcnn) (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n",
      "Installing collected packages: mtcnn\n",
      "Successfully installed mtcnn-0.1.0\n",
      "time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 157685,
     "status": "ok",
     "timestamp": 1603555583789,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "raO3mVLfMibC",
    "outputId": "806b836a-f030-4f4e-b20d-07443f899ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 31kB/s \n",
      "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.32.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.35.1)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 60kB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 48.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (50.3.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.6.20)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n",
      "time: 1min 59s\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 160756,
     "status": "ok",
     "timestamp": 1603555586868,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "1HiBB7Hk1Cr-",
    "outputId": "9de6b3fa-8a91-4afc-b8fa-e23314c028ac"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#import keras\n",
    "#from keras.preprocessing.image import load_img\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, LeakyReLU\n",
    "from keras.layers import Conv2D, AveragePooling2D, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from glob import glob\n",
    "import os\n",
    "from mtcnn import MTCNN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 160751,
     "status": "ok",
     "timestamp": 1603555586870,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "KvutOpBGdpVr",
    "outputId": "8d998a73-68b2-48c6-be7a-9c27eaee9c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 163744,
     "status": "ok",
     "timestamp": 1603555589870,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Hfiio5_KHufK",
    "outputId": "c60f1f7d-c78e-493e-d1c9-5a63020cf6ab"
   },
   "outputs": [],
   "source": [
    "if CLEAN_AUG_DATA == 1:\n",
    "    # cleandata: 清除wiki資料集原本的一些年齡標註錯誤, 並擴增10歲以下與70歲以上的資料\n",
    "    df = pd.read_csv(os.path.join(img_folder_path, 'cleandata.csv'))\n",
    "else:\n",
    "    # 資料集由csv檔案讀入, 也可新增其他的csv檔案來擴充資料\n",
    "    # df = pd.read_csv('drive/My Drive/Tibame_AIoT_Project/Datasets/資料集_IMDB-Wiki/age_gender_wiki.csv')\n",
    "    # df_under10 = pd.read_csv('drive/My Drive/Tibame_AIoT_Project/Datasets/資料集_IMDB-Wiki/age_gender_imdb_under10.csv')\n",
    "    # df_over70 = pd.read_csv('drive/My Drive/Tibame_AIoT_Project/Datasets/資料集_IMDB-Wiki/age_gender_imdb_over70.csv')\n",
    "    df = pd.read_csv(os.path.join(img_folder_path, 'age_gender_wiki.csv'))\n",
    "    df_under10 = pd.read_csv(os.path.join(img_folder_path, 'age_gender_imdb_under10.csv'))\n",
    "    df_over70 = pd.read_csv(os.path.join(img_folder_path, 'age_gender_imdb_over70.csv'))\n",
    "    df = pd.concat([df, df_under10, df_over70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 163741,
     "status": "ok",
     "timestamp": 1603555589873,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "6vj29J-vpoyz",
    "outputId": "01a4bde5-24aa-4e97-a2d6-3188b1eb20ff"
   },
   "outputs": [],
   "source": [
    "#some guys seem to be greater than 100. some of these are paintings. remove these old guys\n",
    "df = df[df['age'] <= 100]\n",
    " \n",
    "#some guys seem to be unborn in the data set\n",
    "df = df[df['age'] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "executionInfo": {
     "elapsed": 164162,
     "status": "ok",
     "timestamp": 1603555590301,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "ZI0vTSSFtF3o",
    "outputId": "09de302c-9439-4f63-b562-ec29ed342346"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_grp</th>\n",
       "      <th>age_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wiki_crop/17/10000217_1981-05-05_2009.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>[20, 30)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wiki_crop/12/100012_1948-07-03_2008.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>[50, 60)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wiki_crop/16/10002116_1971-05-31_2012.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>[40, 50)</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wiki_crop/02/10002702_1960-11-09_2012.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>[50, 60)</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wiki_crop/41/10003541_1937-09-27_1971.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>[30, 40)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2268</th>\n",
       "      <td>imdb_crop/78/nm0498278_rm974293248_1922-12-28_...</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>[70, 110)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>imdb_crop/19/nm0694619_rm3523791616_1942-11-2_...</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>[70, 110)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2270</th>\n",
       "      <td>imdb_crop/19/nm0694619_rm3574123264_1942-11-2_...</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>[70, 110)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2271</th>\n",
       "      <td>imdb_crop/56/nm0792556_rm551744512_1928-6-12_2...</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>[70, 110)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>imdb_crop/92/nm0891092_rm609007616_1923-4-4_20...</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>[70, 110)</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35288 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              full_path  gender  age  \\\n",
       "0             wiki_crop/17/10000217_1981-05-05_2009.jpg       1   27   \n",
       "1               wiki_crop/12/100012_1948-07-03_2008.jpg       1   59   \n",
       "2             wiki_crop/16/10002116_1971-05-31_2012.jpg       0   40   \n",
       "3             wiki_crop/02/10002702_1960-11-09_2012.jpg       0   51   \n",
       "4             wiki_crop/41/10003541_1937-09-27_1971.jpg       1   33   \n",
       "...                                                 ...     ...  ...   \n",
       "2268  imdb_crop/78/nm0498278_rm974293248_1922-12-28_...       1   86   \n",
       "2269  imdb_crop/19/nm0694619_rm3523791616_1942-11-2_...       0   71   \n",
       "2270  imdb_crop/19/nm0694619_rm3574123264_1942-11-2_...       0   71   \n",
       "2271  imdb_crop/56/nm0792556_rm551744512_1928-6-12_2...       1   86   \n",
       "2272  imdb_crop/92/nm0891092_rm609007616_1923-4-4_20...       1   87   \n",
       "\n",
       "        age_grp  age_cls  \n",
       "0      [20, 30)        2  \n",
       "1      [50, 60)        5  \n",
       "2      [40, 50)        4  \n",
       "3      [50, 60)        5  \n",
       "4      [30, 40)        3  \n",
       "...         ...      ...  \n",
       "2268  [70, 110)        7  \n",
       "2269  [70, 110)        7  \n",
       "2270  [70, 110)        7  \n",
       "2271  [70, 110)        7  \n",
       "2272  [70, 110)        7  \n",
       "\n",
       "[35288 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每10歲分一類,70歲以上歸為同一類,共8類\n",
    "df['age_grp'] = pd.cut(df['age'], bins=[0,10,20,30,40,50,60,70,110], right=False)\n",
    "le = LabelEncoder()\n",
    "le.fit(df['age_grp'].astype('str'))\n",
    "df['age_cls'] = le.transform(df['age_grp'].astype('str'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 164154,
     "status": "ok",
     "timestamp": 1603555590302,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "V9bgzkzMLpkX",
    "outputId": "b74fb538-57cb-451d-da87-c6a70e7bf88c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      877\n",
       "1     2125\n",
       "2    11411\n",
       "3     6625\n",
       "4     4681\n",
       "5     3420\n",
       "6     2174\n",
       "7     3975\n",
       "Name: age_cls, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_cls'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 164148,
     "status": "ok",
     "timestamp": 1603555590304,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "ynsfUWLDp4r7",
    "outputId": "92dc7b52-8176-4997-aa8f-076b7ea31b59"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram_age = df['age_cls'].hist(bins=df['age_cls'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 164685,
     "status": "ok",
     "timestamp": 1603555590846,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "SC37vJjo_AB_",
    "outputId": "e24e5f32-ea76-4ba4-ecff-cede00188ec9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram_gender = df['gender'].hist(bins=df['gender'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "executionInfo": {
     "elapsed": 164682,
     "status": "ok",
     "timestamp": 1603555590850,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "HW4fp3zW99L8",
    "outputId": "e4ac687d-00ff-4b2c-8cfb-9919b5662cca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([(0,  [20, 30)),\n",
      "            (0,  [30, 40)),\n",
      "            (0,  [40, 50)),\n",
      "            (0, [70, 110)),\n",
      "            (0,  [10, 20)),\n",
      "            (0,  [50, 60)),\n",
      "            (0,   [0, 10)),\n",
      "            (0,  [60, 70)),\n",
      "            (1,  [20, 30)),\n",
      "            (1,  [30, 40)),\n",
      "            (1,  [40, 50)),\n",
      "            (1, [70, 110)),\n",
      "            (1,  [50, 60)),\n",
      "            (1,  [60, 70)),\n",
      "            (1,  [10, 20)),\n",
      "            (1,   [0, 10))],\n",
      "           names=['gender', 'age_grp'])\n",
      "[3527 1934 1116 1016  940  698  513  403 7884 4691 3565 2959 2722 1771\n",
      " 1185  364]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gender  age_grp  \n",
       "0       [0, 10)       513\n",
       "        [10, 20)      940\n",
       "        [20, 30)     3527\n",
       "        [30, 40)     1934\n",
       "        [40, 50)     1116\n",
       "        [50, 60)      698\n",
       "        [60, 70)      403\n",
       "        [70, 110)    1016\n",
       "1       [0, 10)       364\n",
       "        [10, 20)     1185\n",
       "        [20, 30)     7884\n",
       "        [30, 40)     4691\n",
       "        [40, 50)     3565\n",
       "        [50, 60)     2722\n",
       "        [60, 70)     1771\n",
       "        [70, 110)    2959\n",
       "Name: age_grp, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.groupby(['gender'])['age_grp'].value_counts()\n",
    "print(s.index)\n",
    "print(s.values)\n",
    "cls_cnt = s.sort_index()\n",
    "cls_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 164678,
     "status": "ok",
     "timestamp": 1603555590853,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "0j47XcawO-zR",
    "outputId": "82b2615a-7dff-4d53-90d7-819bafc11459"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 513,  940, 3527, 1934, 1116,  698,  403, 1016,  364, 1185, 7884,\n",
       "       4691, 3565, 2722, 1771, 2959], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cls_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 164674,
     "status": "ok",
     "timestamp": 1603555590854,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Ks6bHLoBLQYx",
    "outputId": "055ee92f-f6b9-4cdf-ffc4-25d7ec48047d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 51 105 428 229 127  74  37 114  33 135 973 573 433 327 208 357]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[ 51 105 428 229 127  74  37 114  33 135 973 573 433 327 208 357]\n"
     ]
    }
   ],
   "source": [
    "per_cls_eval = 100\n",
    "\n",
    "if FULL_DATA == 0:\n",
    "    #因資料不平均, 且多輸出分類器不能用class_weight, 所以只用部分資料來訓練\n",
    "    per_cls_trn = 20 #100\n",
    "    # class:  f0  f1  f2   f3  f4  f5   f6  f7  m0  m1  m2   m3   m4   m5   m6   m7\n",
    "    # start_idx = [0, 600, 600, 600, 0, 600, 600, 0, 0, 600, 600, 600, 600, 600, 600, 600]\n",
    "    start_idx = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    end_idx = np.array(start_idx) + per_cls_trn\n",
    "    print(start_idx, end_idx)\n",
    "else:\n",
    "    # 資料量大, 分成幾群分別訓練(data_bunch_idx = 0 ~ data_bunch-1)\n",
    "    data_bunch = 8\n",
    "    data_bunch_idx = 0       \n",
    "\n",
    "    per_cls_trn = (np.array(cls_cnt) - per_cls_eval) // data_bunch\n",
    "    start_idx = np.zeros(16, dtype=int) + per_cls_trn * data_bunch_idx\n",
    "    end_idx = start_idx + per_cls_trn\n",
    "\n",
    "\n",
    "print(per_cls_trn)\n",
    "print(start_idx)\n",
    "print(end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 164670,
     "status": "ok",
     "timestamp": 1603555590856,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "YSCJlWc444MN",
    "outputId": "580d658e-e4ee-48b6-c19f-fe1a0fda9ecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4204 predict: 1600\n"
     ]
    }
   ],
   "source": [
    "#先用少量資料比較不同模型:\n",
    "#每個類別各取部分資料,用train_test_split來切train and test\n",
    "df_f0 = df[(df['age_cls'] == 0) & (df['gender'] == 0)]\n",
    "df_f1 = df[(df['age_cls'] == 1) & (df['gender'] == 0)]\n",
    "df_f2 = df[(df['age_cls'] == 2) & (df['gender'] == 0)]\n",
    "df_f3 = df[(df['age_cls'] == 3) & (df['gender'] == 0)]\n",
    "df_f4 = df[(df['age_cls'] == 4) & (df['gender'] == 0)]\n",
    "df_f5 = df[(df['age_cls'] == 5) & (df['gender'] == 0)]\n",
    "df_f6 = df[(df['age_cls'] == 6) & (df['gender'] == 0)]\n",
    "df_f7 = df[(df['age_cls'] == 7) & (df['gender'] == 0)]\n",
    "df_m0 = df[(df['age_cls'] == 0) & (df['gender'] == 1)]\n",
    "df_m1 = df[(df['age_cls'] == 1) & (df['gender'] == 1)]\n",
    "df_m2 = df[(df['age_cls'] == 2) & (df['gender'] == 1)]\n",
    "df_m3 = df[(df['age_cls'] == 3) & (df['gender'] == 1)]\n",
    "df_m4 = df[(df['age_cls'] == 4) & (df['gender'] == 1)]\n",
    "df_m5 = df[(df['age_cls'] == 5) & (df['gender'] == 1)]\n",
    "df_m6 = df[(df['age_cls'] == 6) & (df['gender'] == 1)]\n",
    "df_m7 = df[(df['age_cls'] == 7) & (df['gender'] == 1)]\n",
    "\n",
    "# train and val data\n",
    "train_df = pd.concat([\n",
    "    df_f0[start_idx[0]:end_idx[0]], df_f1[start_idx[1]:end_idx[1]], df_f2[start_idx[2]:end_idx[2]], df_f3[start_idx[3]:end_idx[3]], \n",
    "    df_f4[start_idx[4]:end_idx[4]], df_f5[start_idx[5]:end_idx[5]], df_f6[start_idx[6]:end_idx[6]], df_f7[start_idx[7]:end_idx[7]], \n",
    "    df_m0[start_idx[8]:end_idx[8]], df_m1[start_idx[9]:end_idx[9]], df_m2[start_idx[10]:end_idx[10]], df_m3[start_idx[11]:end_idx[11]], \n",
    "    df_m4[start_idx[12]:end_idx[12]], df_m5[start_idx[13]:end_idx[13]], df_m6[start_idx[14]:end_idx[14]], df_m7[start_idx[15]:end_idx[15]]         \n",
    "    ])\n",
    "\"\"\"\n",
    "if FULL_DATA == 1:\n",
    "    每個類別保留最後per_cls_eval筆資料作為evaluate用\n",
    "    train_df = pd.concat([\n",
    "        df_f0[:-per_cls_eval], df_f1[:-per_cls_eval], df_f2[:-per_cls_eval], df_f3[:-per_cls_eval], \n",
    "        df_f4[:-per_cls_eval], df_f5[:-per_cls_eval], df_f6[:-per_cls_eval], df_f7[:-per_cls_eval], \n",
    "        df_m0[:-per_cls_eval], df_m1[:-per_cls_eval], df_m2[:-per_cls_eval], df_m3[:-per_cls_eval], \n",
    "        df_m4[:-per_cls_eval], df_m5[:-per_cls_eval], df_m6[:-per_cls_eval], df_m7[:-per_cls_eval]         \n",
    "        ])   \n",
    "\n",
    "\n",
    "    #模擬imbalance data\n",
    "    # train_df = pd.concat([\n",
    "    #     df_f0[:10], df_f1[:20], df_f2[:30], df_f3[:40], \n",
    "    #     df_f4[:40], df_f5[:30], df_f6[:20], df_f7[:10], \n",
    "    #     df_m0[:10], df_m1[:20], df_m2[:30], df_m3[:40], \n",
    "    #     df_m4[:40], df_m5[:30], df_m6[:20], df_m7[:10]         \n",
    "    #     ])            \n",
    "else:    \n",
    "    #每個類別取同樣數量的資料來訓練\n",
    "    train_df = pd.concat([\n",
    "        df_f0[start_idx[0]:end_idx[0]], df_f1[start_idx[1]:end_idx[1]], df_f2[start_idx[2]:end_idx[2]], df_f3[start_idx[3]:end_idx[3]], \n",
    "        df_f4[start_idx[4]:end_idx[4]], df_f5[start_idx[5]:end_idx[5]], df_f6[start_idx[6]:end_idx[6]], df_f7[start_idx[7]:end_idx[7]], \n",
    "        df_m0[start_idx[8]:end_idx[8]], df_m1[start_idx[9]:end_idx[9]], df_m2[start_idx[10]:end_idx[10]], df_m3[start_idx[11]:end_idx[11]], \n",
    "        df_m4[start_idx[12]:end_idx[12]], df_m5[start_idx[13]:end_idx[13]], df_m6[start_idx[14]:end_idx[14]], df_m7[start_idx[15]:end_idx[15]]         \n",
    "        ])\n",
    "\"\"\"\n",
    "    \n",
    "# evaluate data: 每個類別保留最後per_cls_eval筆資料作為evaluate用\n",
    "eval_df = pd.concat([\n",
    "        df_f0[-per_cls_eval:], df_f1[-per_cls_eval:], df_f2[-per_cls_eval:], df_f3[-per_cls_eval:], \n",
    "        df_f4[-per_cls_eval:], df_f5[-per_cls_eval:], df_f6[-per_cls_eval:], df_f7[-per_cls_eval:],\n",
    "        df_m0[-per_cls_eval:], df_m1[-per_cls_eval:], df_m2[-per_cls_eval:], df_m3[-per_cls_eval:], \n",
    "        df_m4[-per_cls_eval:], df_m5[-per_cls_eval:], df_m6[-per_cls_eval:], df_m7[-per_cls_eval:]         \n",
    "        ])\n",
    "x_eval = np.array(eval_df['full_path'])\n",
    "# 先把模型的兩個輸出的答案合併\n",
    "y_eval = np.array(pd.concat([eval_df['age_cls'], eval_df['gender']], axis=1))\n",
    "print(\"train:\", len(train_df), \"predict:\", len(eval_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 164666,
     "status": "ok",
     "timestamp": 1603555590856,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "PQi3zwjxagcW",
    "outputId": "78b0fe92-0665-47c4-984c-c51f26927f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_crop/68/1403768_1970-11-09_2006.jpg wiki_crop/26/810826_1941-07-28_2015.jpg [3 0] [7 1]\n",
      "(3363,) (841,) (3363, 2) (841, 2)\n"
     ]
    }
   ],
   "source": [
    "# 處理答案 把它轉成one-hot (後面再做)\n",
    "# y_train_category = to_categorical(df['age_cls'], num_classes=8)\n",
    "\n",
    "# 2個輸出: age, gender\n",
    "y_df = pd.concat([pd.DataFrame(train_df['age_cls']), pd.DataFrame(train_df['gender'])], axis=1)\n",
    "#y_df = pd.DataFrame(train_df['age_cls'])\n",
    "\n",
    "# 切分訓練data\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(train_df['full_path']), np.array(y_df), test_size=0.2, random_state=0)\n",
    "\n",
    "print(x_train[0], x_test[0], y_train[0], y_test[0])\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 166908,
     "status": "ok",
     "timestamp": 1603555593103,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "FZJoKwSbtpb8",
    "outputId": "7f7a6e5e-fa8e-48f4-8553-473651d66975"
   },
   "outputs": [],
   "source": [
    "detector = MTCNN()\n",
    "#feature_extractor = load_model(os.path.join(model_folder_path, 'facenet_keras.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 175542,
     "status": "ok",
     "timestamp": 1603555601743,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "DypYAJ7cJlrC",
    "outputId": "0a971533-861b-43b8-d452-b0134bd18347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_resnet50.h5\n",
      "94699520/94694792 [==============================] - 22s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# VGGFace: https://github.com/rcmalli/keras-vggface\n",
    "!pip install keras_vggface\n",
    "!pip install keras_applications\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "feature_extractor = VGGFace(model='resnet50', include_top=False, \n",
    "            input_shape=(224, 224, 3), pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 175916,
     "status": "ok",
     "timestamp": 1603555602123,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "fOgsPn1i3E21",
    "outputId": "c1356175-f63d-4fe5-cca3-d0107c537593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vggface_resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 55, 55, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 55, 55, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,561,152\n",
      "Trainable params: 23,508,032\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 175912,
     "status": "ok",
     "timestamp": 1603555602125,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "7iJ6QxgZ_8iI",
    "outputId": "3a743b68-167a-4a39-e81c-67e682336723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 55, 55, 64)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 55, 55, 256)  0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 55, 55, 256)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 55, 55, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 28, 28, 512)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 28, 28, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 28, 28, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 28, 28, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 14, 14, 1024) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 14, 14, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 14, 14, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 14, 14, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 14, 14, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 14, 14, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 7, 7, 2048)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 7, 7, 2048)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
      "                                                                 activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 7, 7, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 2048)         8192        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2048)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          1049088     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 512)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 128)          65664       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "age (Dense)                     (None, 8)            1032        dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 24,687,688\n",
      "Trainable params: 1,121,160\n",
      "Non-trainable params: 23,566,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 固定pre-train model的參數\n",
    "for lyr in feature_extractor.layers:\n",
    "    lyr.trainable = False\n",
    "\n",
    "# BN\n",
    "x = BatchNormalization()(feature_extractor.output)    \n",
    "    \n",
    "# MLP    \n",
    "# x = Flatten()(x)\n",
    "\n",
    "if DROP_RATE != 0:\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "x = Dense(units=512, kernel_regularizer='l1_l2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "if DROP_RATE != 0:\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "x = Dense(units=128, kernel_regularizer='l1_l2')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU()(x)\n",
    "\n",
    "if DROP_RATE != 0:\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "age = Dense(units=8, activation='softmax', name='age', kernel_regularizer='l1_l2')(x)\n",
    "#gender = Dense(units=2, activation='softmax', name='gender', kernel_regularizer='l1_l2')(x)\n",
    "\n",
    "inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# 2個輸出: age, gender\n",
    "# age_gender_model = Model(inputs=feature_extractor.input, outputs=[age, gender])   \n",
    "# age_gender_model.summary()\n",
    "\n",
    "# 1個輸出: age\n",
    "age_model = Model(inputs=feature_extractor.input, outputs=[age])   \n",
    "age_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1mHRc7zbH5U4CopR8pJgaFltC7k5dk42f"
    },
    "executionInfo": {
     "elapsed": 180468,
     "status": "ok",
     "timestamp": 1603555606686,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Hev5u-W9MLXn",
    "outputId": "d12bf4be-a77f-43cd-935b-fc096189b3bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(age_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 181631,
     "status": "ok",
     "timestamp": 1603555607854,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "Bs6tioz-AvmI",
    "outputId": "73fb90f9-f9bd-42b5-f38a-fe228ef089e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "age_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy') # 1個輸出: age\n",
    "\n",
    "age_model = load_model(os.path.join(model_folder_path,'03-5_resnet_mlp512-128_bs32_ep5_0.4468.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 181627,
     "status": "ok",
     "timestamp": 1603555607855,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "D-Mn0wTy5Bze",
    "outputId": "0668184a-f2a9-4eee-c951-8f833528b1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.16 ms\n"
     ]
    }
   ],
   "source": [
    "# # 資料預處理 for facenet?\n",
    "# # Standardization\n",
    "# def preprocess(imgs): \n",
    "#     for i in range(imgs.shape[0]):\n",
    "#         # standardization\n",
    "#         img = imgs[i]\n",
    "#         mean, std = img.mean(), img.std()\n",
    "#         img = (img - mean) / std\n",
    "#         imgs[i] = img\n",
    "#     return imgs\n",
    "# # Normalization\n",
    "# def normalize(img):\n",
    "#     return img / 255.\n",
    "\n",
    "# # -1 <= x <= 1\n",
    "# def preprocess_1(imgs):\n",
    "#     x = np.array(imgs, dtype = float)\n",
    "#     x /= 127.5\n",
    "#     x -= 1.\n",
    "#     return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 182299,
     "status": "ok",
     "timestamp": 1603555608533,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "trDkpzl5XBky",
    "outputId": "fad67585-1e46-4990-b846-886b5ac151ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 529 ms\n"
     ]
    }
   ],
   "source": [
    "# 在 DataGenerator 做 real time 資料擴增\n",
    "import imgaug.augmenters as iaa\n",
    "import imgaug as ia\n",
    "# augmentation\n",
    "seq = iaa.Sequential([\n",
    "    #iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n",
    "    iaa.Fliplr(0.5), # horizontally flip 50% of the images\n",
    "    iaa.GaussianBlur(sigma=(0, 3.0)), # blur images with a sigma of 0 to 3.0\n",
    "    #iaa.Flipud(0.5),\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.7, 1.3), \"y\": (0.7, 1.3)},\n",
    "        rotate=(-15, 15),\n",
    "        # mode=ia.ALL, # edge, reflect, symmetric, warp, constant\n",
    "        # shear=(-16,16)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 182296,
     "status": "ok",
     "timestamp": 1603555608534,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "evAgdxQ20ICI",
    "outputId": "2ded48a9-179f-4fe6-f340-b302440ec20c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.39 ms\n"
     ]
    }
   ],
   "source": [
    "# detect face\n",
    "def detect_faces(img):\n",
    "    face_imgs = []\n",
    "\n",
    "    results = detector.detect_faces(img)\n",
    "    # extract the bounding box from the first face\n",
    "    # print('# of faces: ', len(results))\n",
    "    for i in range(len(results)):\n",
    "        x1, y1, width, height = results[i]['box']\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        patch = img[y1:y2, x1:x2] # crop face\n",
    "        face_imgs.append(patch)\n",
    "     \n",
    "    return face_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 182291,
     "status": "ok",
     "timestamp": 1603555608535,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "c8gr0xxA9NwP",
    "outputId": "9e797a93-87ca-4284-cad1-3d72ab64c5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates data for Keras\n",
    "    ref: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 paths,\n",
    "                 y_cls,\n",
    "                 batch_size,\n",
    "                 #num_classes,\n",
    "                 shuffle=False,\n",
    "                 augment=False):\n",
    "        self.paths = paths\n",
    "        self.y_cls = y_cls\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        #self.num_classes = num_classes\n",
    "        self.indexes = np.arange(len(self.paths))\n",
    "        #self.class_map = {'0':0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7}\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'number of batches per epoch'\n",
    "        return int(np.ceil(len(self.paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        idxs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        batch_paths = [self.paths[i] for i in idxs]\n",
    "        batch_y = [self.y_cls[i] for i in idxs]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(batch_paths, batch_y)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, paths, y_cls):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples\n",
    "        \"\"\"\n",
    "        # X = np.empty((len(paths), IMG_SIZE, IMG_SIZE, 3), dtype=np.float32)\n",
    "        # y = np.empty((len(paths), self.num_classes), dtype=np.float32)\n",
    "\n",
    "        x_ori, x_norm, y_age, y_gender = [], [], [], []\n",
    "\n",
    "        for i, path in enumerate(paths):\n",
    "            #print(\"idx:\", i, \"cls:\", y_cls[i], path)\n",
    "        \n",
    "            # 讀取圖片,切下臉的部分,並使用借來的模型的預處理方式來作預處理 \n",
    "            try:          \n",
    "                img = cv2.imread(os.path.join(img_folder_path,path))[:,:,::-1]\n",
    "            except:\n",
    "                print('imread failed:', path)\n",
    "                continue \n",
    "\n",
    "            if self.augment:\n",
    "                img_aug = seq.augment_image(img)\n",
    "                # print(img_aug.shape)\n",
    "                # plt.figure(figsize=(10, 10))\n",
    "                # plt.subplot(1, 2, 1)\n",
    "                # plt.imshow(img)\n",
    "                # plt.subplot(1, 2, 2)\n",
    "                # plt.imshow(img_aug)\n",
    "                # plt.show()\n",
    "                \n",
    "                img = img_aug\n",
    "\n",
    "\n",
    "            faces = detect_faces(img)\n",
    "            if len(faces) == 0 or faces[0].shape[0] == 0 or faces[0].shape[1] == 0:\n",
    "                print(' No face')\n",
    "                continue\n",
    "            #print(faces[0].shape)   \n",
    "            img_crop = cv2.resize(faces[0], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            # 使用借來的模型的預處理方式來作預處理\n",
    "            img_pre = preprocess_input(np.array(img_crop,dtype=float))\n",
    "            \n",
    "            # 把原圖留下來\n",
    "            x_ori.append(img)\n",
    "            x_norm.append(img_pre)\n",
    "            # 2個輸出: age, gender\n",
    "            y_age.append(y_cls[i][0])\n",
    "            y_gender.append(y_cls[i][1])\n",
    "\n",
    "            \n",
    "\n",
    "        # print(\"len(image_data)\",len(x_ori))\n",
    "        # plt.figure(figsize=(10, 40))\n",
    "        # for j,m in enumerate(x_ori):\n",
    "        #     plt.subplot(1, BATCH_SIZE, (j%BATCH_SIZE)+1)\n",
    "        #     plt.title(\"idx:{} y_cls:{}\".format(i_batch+j, y_cls[i_batch+j]))\n",
    "        #     plt.axis(\"off\")\n",
    "        #     plt.imshow(m)\n",
    "        # plt.show() \n",
    "\n",
    "        \n",
    "        # 2個輸出: age, gender  \n",
    "        # print(type(y_age), len(y_age), y_age[:8])\n",
    "        # print(type(y_gender), len(y_gender), y_gender[:8])\n",
    "        y_age_category = to_categorical(y_age, num_classes=8) \n",
    "        y_gender_category = to_categorical(y_gender, num_classes=2) \n",
    "        # print(y_age_category)\n",
    "        # print(y_gender_category)\n",
    "        x_input = {'input_4':np.array(x_norm)}\n",
    "        # 2個輸出: age, gender  \n",
    "        y_category = {'age':np.array(y_age_category), 'gender':np.array(y_gender_category)}\n",
    "        # 1個輸出: age\n",
    "        y_category = np.array(y_age_category)\n",
    "        # print(type(np.array(x_norm)), np.array(x_norm).shape)\n",
    "        # print(type(y_category), np.array(y_age_category), np.array(y_gender_category))\n",
    "\n",
    "        #yield x_input, y_category\n",
    "        return x_input, y_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 182286,
     "status": "ok",
     "timestamp": 1603555608535,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "3DidZIPzKHzS",
    "outputId": "16910820-a508-4626-ab85-0a7c30bd9473"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.09 ms\n"
     ]
    }
   ],
   "source": [
    "# 用generator產生資料\n",
    "generator_train = DataGenerator(x_train, y_train, batch_size=BATCH_SIZE, augment=True)\n",
    "generator_test = DataGenerator(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "#type(generator_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 182282,
     "status": "ok",
     "timestamp": 1603555608536,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "kJy0oQQVXyls",
    "outputId": "1ac2babe-2eb7-41d1-8b11-37d333c34f3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 4, ..., 2, 3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.96 ms\n"
     ]
    }
   ],
   "source": [
    "y_train[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "executionInfo": {
     "elapsed": 182278,
     "status": "ok",
     "timestamp": 1603555608536,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "ZEWln4Tua3dg",
    "outputId": "c68f7e88-8205-4924-fd7a-00ab2fb143ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  69  187 1114  652  449  314  209  369]\n",
      "class_weight {0: 6.092391304347826, 1: 2.247994652406417, 2: 0.3773563734290844, 3: 0.6447469325153374, 4: 0.9362472160356347, 5: 1.3387738853503186, 6: 2.0113636363636362, 7: 1.1392276422764227}\n",
      "time: 4.81 ms\n"
     ]
    }
   ],
   "source": [
    "# 計算  class_weight\n",
    "data_count = np.unique(y_train[:,0], return_counts=True)[1]\n",
    "print(data_count)\n",
    "num_classes=8\n",
    "age_weights = (1/data_count)*np.sum(data_count)/num_classes\n",
    "class_weight = {i: w for i, w in enumerate(age_weights)}\n",
    "print('class_weight', class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jc_L3HTVMio7",
    "outputId": "ec07f73f-0ed6-4db7-e9c6-14647438aed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/106 [..............................] - ETA: 9:11 - loss: 10.3459 - accuracy: 0.3438 No face\n",
      " No face\n",
      "  5/106 [>.............................] - ETA: 13:40 - loss: 10.2344 - accuracy: 0.3861 No face\n",
      " 11/106 [==>...........................] - ETA: 13:50 - loss: 9.9228 - accuracy: 0.3983 No face\n",
      " 12/106 [==>...........................] - ETA: 13:50 - loss: 9.8950 - accuracy: 0.3921 No face\n",
      " No face\n",
      " 13/106 [==>...........................] - ETA: 13:45 - loss: 9.8256 - accuracy: 0.3976 No face\n",
      " No face\n",
      " 18/106 [====>.........................] - ETA: 13:14 - loss: 9.4867 - accuracy: 0.3944 No face\n",
      " No face\n",
      " 22/106 [=====>........................] - ETA: 12:42 - loss: 9.3014 - accuracy: 0.3934 No face\n",
      " No face\n",
      " 29/106 [=======>......................] - ETA: 11:45 - loss: 9.0527 - accuracy: 0.3897 No face\n",
      " 30/106 [=======>......................] - ETA: 11:38 - loss: 9.0181 - accuracy: 0.3854 No face\n",
      " 31/106 [=======>......................] - ETA: 11:30 - loss: 9.0043 - accuracy: 0.3824 No face\n",
      " 33/106 [========>.....................] - ETA: 11:13 - loss: 8.9650 - accuracy: 0.3881 No face\n",
      " 34/106 [========>.....................] - ETA: 11:03 - loss: 8.9500 - accuracy: 0.3862 No face\n",
      " No face\n",
      " 39/106 [==========>...................] - ETA: 10:21 - loss: 8.9363 - accuracy: 0.3837 No face\n",
      " 40/106 [==========>...................] - ETA: 10:13 - loss: 8.9274 - accuracy: 0.3822 No face\n",
      " 42/106 [==========>...................] - ETA: 9:54 - loss: 8.9436 - accuracy: 0.3807  No face\n",
      " 45/106 [===========>..................] - ETA: 9:17 - loss: 8.9512 - accuracy: 0.3813 No face\n",
      " 47/106 [============>.................] - ETA: 8:59 - loss: 8.9613 - accuracy: 0.3833 No face\n",
      " 48/106 [============>.................] - ETA: 8:51 - loss: 8.9651 - accuracy: 0.3868 No face\n",
      " 52/106 [=============>................] - ETA: 8:16 - loss: 9.0073 - accuracy: 0.3842 No face\n",
      " 54/106 [==============>...............] - ETA: 7:59 - loss: 9.0485 - accuracy: 0.3853 No face\n",
      " 57/106 [===============>..............] - ETA: 7:32 - loss: 9.1257 - accuracy: 0.3844 No face\n",
      " No face\n",
      " No face\n",
      " 58/106 [===============>..............] - ETA: 7:23 - loss: 9.1397 - accuracy: 0.3843 No face\n",
      " 61/106 [================>.............] - ETA: 6:56 - loss: 9.1863 - accuracy: 0.3851 No face\n",
      " 62/106 [================>.............] - ETA: 6:47 - loss: 9.2042 - accuracy: 0.3851 No face\n",
      " No face\n",
      " No face\n",
      " 64/106 [=================>............] - ETA: 6:29 - loss: 9.2250 - accuracy: 0.3874 No face\n",
      " 68/106 [==================>...........] - ETA: 5:52 - loss: 9.2456 - accuracy: 0.3873 No face\n",
      " 69/106 [==================>...........] - ETA: 5:43 - loss: 9.2406 - accuracy: 0.3873 No face\n",
      " 74/106 [===================>..........] - ETA: 4:57 - loss: 9.2250 - accuracy: 0.3875 No face\n",
      " 76/106 [====================>.........] - ETA: 4:39 - loss: 9.2010 - accuracy: 0.3882 No face\n",
      " 85/106 [=======================>......] - ETA: 3:16 - loss: 9.1600 - accuracy: 0.3854 No face\n",
      " 87/106 [=======================>......] - ETA: 2:57 - loss: 9.1716 - accuracy: 0.3878 No face\n",
      " 89/106 [========================>.....] - ETA: 2:38 - loss: 9.1961 - accuracy: 0.3898 No face\n",
      " No face\n",
      " 90/106 [========================>.....] - ETA: 2:29 - loss: 9.2100 - accuracy: 0.3903 No face\n",
      " No face\n",
      " 92/106 [=========================>....] - ETA: 2:10 - loss: 9.2338 - accuracy: 0.3909 No face\n",
      " No face\n",
      " 97/106 [==========================>...] - ETA: 1:24 - loss: 9.2745 - accuracy: 0.3907 No face\n",
      " 98/106 [==========================>...] - ETA: 1:14 - loss: 9.2774 - accuracy: 0.3910 No face\n",
      " 99/106 [===========================>..] - ETA: 1:05 - loss: 9.2843 - accuracy: 0.3887 No face\n",
      " No face\n",
      "100/106 [===========================>..] - ETA: 56s - loss: 9.2870 - accuracy: 0.3888  No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.2870 - accuracy: 0.3836 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1254s 12s/step - loss: 9.2870 - accuracy: 0.3836 - val_loss: 8.8437 - val_accuracy: 0.4504 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "  2/106 [..............................] - ETA: 8:12 - loss: 8.9395 - accuracy: 0.4062 No face\n",
      " No face\n",
      " No face\n",
      "  5/106 [>.............................] - ETA: 9:59 - loss: 8.8917 - accuracy: 0.3594  No face\n",
      "  6/106 [>.............................] - ETA: 10:55 - loss: 8.8944 - accuracy: 0.3396 No face\n",
      " No face\n",
      "  9/106 [=>............................] - ETA: 12:15 - loss: 8.8566 - accuracy: 0.3399 No face\n",
      " 10/106 [=>............................] - ETA: 12:22 - loss: 8.8660 - accuracy: 0.3380 No face\n",
      " No face\n",
      " 11/106 [==>...........................] - ETA: 12:30 - loss: 8.8615 - accuracy: 0.3312 No face\n",
      " No face\n",
      " 14/106 [==>...........................] - ETA: 12:47 - loss: 8.8964 - accuracy: 0.3407 No face\n",
      " 19/106 [====>.........................] - ETA: 12:28 - loss: 8.8048 - accuracy: 0.3580 No face\n",
      " 23/106 [=====>........................] - ETA: 12:08 - loss: 8.7367 - accuracy: 0.3386 No face\n",
      " 24/106 [=====>........................] - ETA: 12:00 - loss: 8.7112 - accuracy: 0.3366 No face\n",
      " 25/106 [======>.......................] - ETA: 11:55 - loss: 8.6807 - accuracy: 0.3413 No face\n",
      " 27/106 [======>.......................] - ETA: 11:41 - loss: 8.6624 - accuracy: 0.3358 No face\n",
      " 29/106 [=======>......................] - ETA: 11:26 - loss: 8.6655 - accuracy: 0.3401 No face\n",
      " No face\n",
      " No face\n",
      " 31/106 [=======>......................] - ETA: 11:10 - loss: 8.6655 - accuracy: 0.3404 No face\n",
      " 32/106 [========>.....................] - ETA: 11:01 - loss: 8.6594 - accuracy: 0.3398 No face\n",
      " 33/106 [========>.....................] - ETA: 10:53 - loss: 8.6603 - accuracy: 0.3453 No face\n",
      " 37/106 [=========>....................] - ETA: 10:22 - loss: 8.7395 - accuracy: 0.3542 No face\n",
      " 40/106 [==========>...................] - ETA: 9:57 - loss: 8.8111 - accuracy: 0.3610  No face\n",
      " 41/106 [==========>...................] - ETA: 9:49 - loss: 8.8470 - accuracy: 0.3593 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      " 42/106 [==========>...................] - ETA: 9:42 - loss: 8.8727 - accuracy: 0.3624 No face\n",
      " 48/106 [============>.................] - ETA: 8:53 - loss: 9.0605 - accuracy: 0.3724 No face\n",
      " No face\n",
      " 49/106 [============>.................] - ETA: 8:44 - loss: 9.0810 - accuracy: 0.3743 No face\n",
      " 52/106 [=============>................] - ETA: 8:18 - loss: 9.1453 - accuracy: 0.3714 No face\n",
      " No face\n",
      " 61/106 [================>.............] - ETA: 6:56 - loss: 9.2897 - accuracy: 0.3649 No face\n",
      " 64/106 [=================>............] - ETA: 6:29 - loss: 9.3010 - accuracy: 0.3661 No face\n",
      " No face\n",
      " 65/106 [=================>............] - ETA: 6:20 - loss: 9.3026 - accuracy: 0.3651 No face\n",
      " 67/106 [=================>............] - ETA: 6:02 - loss: 9.2896 - accuracy: 0.3671 No face\n",
      " 72/106 [===================>..........] - ETA: 5:16 - loss: 9.2611 - accuracy: 0.3655 No face\n",
      " 75/106 [====================>.........] - ETA: 4:48 - loss: 9.2326 - accuracy: 0.3644 No face\n",
      " 76/106 [====================>.........] - ETA: 4:39 - loss: 9.2282 - accuracy: 0.3659 No face\n",
      " 77/106 [====================>.........] - ETA: 4:30 - loss: 9.2199 - accuracy: 0.3658 No face\n",
      " 87/106 [=======================>......] - ETA: 2:58 - loss: 9.1763 - accuracy: 0.3630 No face\n",
      " No face\n",
      " 88/106 [=======================>......] - ETA: 2:48 - loss: 9.1751 - accuracy: 0.3648 No face\n",
      " 97/106 [==========================>...] - ETA: 1:25 - loss: 9.1964 - accuracy: 0.3580 No face\n",
      " No face\n",
      " 98/106 [==========================>...] - ETA: 1:15 - loss: 9.1980 - accuracy: 0.3594 No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 9.2518 - accuracy: 0.3552 No face\n",
      "105/106 [============================>.] - ETA: 9s - loss: 9.2615 - accuracy: 0.3558  No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.2653 - accuracy: 0.3561 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1273s 12s/step - loss: 9.2653 - accuracy: 0.3561 - val_loss: 10.0409 - val_accuracy: 0.3943 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "  1/106 [..............................] - ETA: 0s - loss: 9.6728 - accuracy: 0.4062 No face\n",
      "  2/106 [..............................] - ETA: 8:22 - loss: 9.9034 - accuracy: 0.3810 No face\n",
      " No face\n",
      "  3/106 [..............................] - ETA: 10:53 - loss: 9.7568 - accuracy: 0.3763 No face\n",
      " No face\n",
      "  8/106 [=>............................] - ETA: 13:38 - loss: 9.6245 - accuracy: 0.3984 No face\n",
      " 12/106 [==>...........................] - ETA: 14:07 - loss: 9.7664 - accuracy: 0.3995 No face\n",
      " No face\n",
      " 14/106 [==>...........................] - ETA: 14:06 - loss: 9.7567 - accuracy: 0.3932 No face\n",
      " 15/106 [===>..........................] - ETA: 14:00 - loss: 9.7744 - accuracy: 0.3949 No face\n",
      " 21/106 [====>.........................] - ETA: 13:25 - loss: 9.8512 - accuracy: 0.3731 No face\n",
      " 22/106 [=====>........................] - ETA: 13:18 - loss: 9.8533 - accuracy: 0.3737 No face\n",
      " No face\n",
      " 26/106 [======>.......................] - ETA: 12:41 - loss: 9.8504 - accuracy: 0.3883 No face\n",
      " No face\n",
      " 27/106 [======>.......................] - ETA: 12:30 - loss: 9.8489 - accuracy: 0.3934 No face\n",
      " 28/106 [======>.......................] - ETA: 12:19 - loss: 9.8453 - accuracy: 0.3943 No face\n",
      " 29/106 [=======>......................] - ETA: 12:09 - loss: 9.8322 - accuracy: 0.3985 No face\n",
      " 35/106 [========>.....................] - ETA: 11:12 - loss: 9.7148 - accuracy: 0.3857 No face\n",
      " 36/106 [=========>....................] - ETA: 11:01 - loss: 9.6862 - accuracy: 0.3839 No face\n",
      " 40/106 [==========>...................] - ETA: 10:24 - loss: 9.5719 - accuracy: 0.3849 No face\n",
      " 42/106 [==========>...................] - ETA: 10:06 - loss: 9.5269 - accuracy: 0.3847 No face\n",
      " 47/106 [============>.................] - ETA: 9:18 - loss: 9.4496 - accuracy: 0.3839 No face\n",
      " 48/106 [============>.................] - ETA: 9:09 - loss: 9.4512 - accuracy: 0.3807 No face\n",
      " 51/106 [=============>................] - ETA: 8:41 - loss: 9.4605 - accuracy: 0.3831 No face\n",
      " 52/106 [=============>................] - ETA: 8:31 - loss: 9.4731 - accuracy: 0.3819 No face\n",
      " No face\n",
      " 54/106 [==============>...............] - ETA: 8:12 - loss: 9.4876 - accuracy: 0.3857 No face\n",
      " 61/106 [================>.............] - ETA: 7:05 - loss: 9.5021 - accuracy: 0.3846 No face\n",
      " 62/106 [================>.............] - ETA: 6:56 - loss: 9.5028 - accuracy: 0.3841 No face\n",
      " 71/106 [===================>..........] - ETA: 5:27 - loss: 9.4995 - accuracy: 0.3782 No face\n",
      " No face\n",
      " 74/106 [===================>..........] - ETA: 5:00 - loss: 9.4972 - accuracy: 0.3771 No face\n",
      " 75/106 [====================>.........] - ETA: 4:51 - loss: 9.4935 - accuracy: 0.3785 No face\n",
      " 77/106 [====================>.........] - ETA: 4:32 - loss: 9.4883 - accuracy: 0.3773 No face\n",
      " No face\n",
      " No face\n",
      " 79/106 [=====================>........] - ETA: 4:14 - loss: 9.4805 - accuracy: 0.3773 No face\n",
      " No face\n",
      " No face\n",
      " 81/106 [=====================>........] - ETA: 3:55 - loss: 9.4713 - accuracy: 0.3809 No face\n",
      " 85/106 [=======================>......] - ETA: 3:18 - loss: 9.4435 - accuracy: 0.3800 No face\n",
      " 87/106 [=======================>......] - ETA: 3:00 - loss: 9.4303 - accuracy: 0.3782 No face\n",
      " 91/106 [========================>.....] - ETA: 2:23 - loss: 9.4172 - accuracy: 0.3782 No face\n",
      " 92/106 [=========================>....] - ETA: 2:13 - loss: 9.4108 - accuracy: 0.3779 No face\n",
      " 94/106 [=========================>....] - ETA: 1:54 - loss: 9.4013 - accuracy: 0.3773 No face\n",
      " 98/106 [==========================>...] - ETA: 1:16 - loss: 9.3803 - accuracy: 0.3760 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.3309 - accuracy: 0.3764 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1306s 12s/step - loss: 9.3309 - accuracy: 0.3764 - val_loss: 8.6197 - val_accuracy: 0.4038 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "  2/106 [..............................] - ETA: 8:08 - loss: 8.7860 - accuracy: 0.2969 No face\n",
      "  4/106 [>.............................] - ETA: 12:59 - loss: 8.5820 - accuracy: 0.2913 No face\n",
      "  5/106 [>.............................] - ETA: 13:35 - loss: 8.5658 - accuracy: 0.2975 No face\n",
      "  6/106 [>.............................] - ETA: 14:08 - loss: 8.4875 - accuracy: 0.3333 No face\n",
      " 10/106 [=>............................] - ETA: 14:40 - loss: 8.4848 - accuracy: 0.3354 No face\n",
      " 13/106 [==>...........................] - ETA: 14:35 - loss: 8.4835 - accuracy: 0.3358 No face\n",
      " 14/106 [==>...........................] - ETA: 14:30 - loss: 8.4850 - accuracy: 0.3348 No face\n",
      " 19/106 [====>.........................] - ETA: 14:01 - loss: 8.5083 - accuracy: 0.3378 No face\n",
      " 21/106 [====>.........................] - ETA: 13:46 - loss: 8.5407 - accuracy: 0.3419 No face\n",
      " 27/106 [======>.......................] - ETA: 13:00 - loss: 8.6880 - accuracy: 0.3614 No face\n",
      " 29/106 [=======>......................] - ETA: 12:43 - loss: 8.7406 - accuracy: 0.3551 No face\n",
      " No face\n",
      " 32/106 [========>.....................] - ETA: 12:15 - loss: 8.7932 - accuracy: 0.3528 No face\n",
      " 34/106 [========>.....................] - ETA: 11:56 - loss: 8.8303 - accuracy: 0.3563 No face\n",
      " 35/106 [========>.....................] - ETA: 11:48 - loss: 8.8445 - accuracy: 0.3562 No face\n",
      " 36/106 [=========>....................] - ETA: 11:39 - loss: 8.8850 - accuracy: 0.3553 No face\n",
      " 38/106 [=========>....................] - ETA: 11:20 - loss: 8.9420 - accuracy: 0.3600 No face\n",
      " 39/106 [==========>...................] - ETA: 11:12 - loss: 8.9732 - accuracy: 0.3566 No face\n",
      " 40/106 [==========>...................] - ETA: 11:02 - loss: 9.0142 - accuracy: 0.3566 No face\n",
      " 41/106 [==========>...................] - ETA: 10:53 - loss: 9.0556 - accuracy: 0.3565 No face\n",
      " No face\n",
      " 42/106 [==========>...................] - ETA: 10:43 - loss: 9.1011 - accuracy: 0.3575 No face\n",
      " No face\n",
      " 45/106 [===========>..................] - ETA: 10:15 - loss: 9.2105 - accuracy: 0.3592 No face\n",
      " 47/106 [============>.................] - ETA: 9:56 - loss: 9.2595 - accuracy: 0.3628  No face\n",
      " 48/106 [============>.................] - ETA: 9:46 - loss: 9.2776 - accuracy: 0.3647 No face\n",
      " 52/106 [=============>................] - ETA: 9:06 - loss: 9.3256 - accuracy: 0.3608 No face\n",
      " No face\n",
      " 53/106 [==============>...............] - ETA: 8:56 - loss: 9.3305 - accuracy: 0.3645 No face\n",
      " No face\n",
      " 57/106 [===============>..............] - ETA: 8:17 - loss: 9.3628 - accuracy: 0.3718 No face\n",
      " 59/106 [===============>..............] - ETA: 7:56 - loss: 9.3711 - accuracy: 0.3737 No face\n",
      " 61/106 [================>.............] - ETA: 7:37 - loss: 9.3755 - accuracy: 0.3750 No face\n",
      " 63/106 [================>.............] - ETA: 7:17 - loss: 9.3648 - accuracy: 0.3777 No face\n",
      " 64/106 [=================>............] - ETA: 7:06 - loss: 9.3614 - accuracy: 0.3774 No face\n",
      " 66/106 [=================>............] - ETA: 6:46 - loss: 9.3510 - accuracy: 0.3779 No face\n",
      " 67/106 [=================>............] - ETA: 6:36 - loss: 9.3519 - accuracy: 0.3786 No face\n",
      " 71/106 [===================>..........] - ETA: 5:56 - loss: 9.3337 - accuracy: 0.3785 No face\n",
      " 72/106 [===================>..........] - ETA: 5:46 - loss: 9.3290 - accuracy: 0.3769 No face\n",
      " No face\n",
      " 74/106 [===================>..........] - ETA: 5:25 - loss: 9.3210 - accuracy: 0.3771 No face\n",
      " 77/106 [====================>.........] - ETA: 4:55 - loss: 9.2956 - accuracy: 0.3760 No face\n",
      " 81/106 [=====================>........] - ETA: 4:15 - loss: 9.2517 - accuracy: 0.3776 No face\n",
      " 87/106 [=======================>......] - ETA: 3:13 - loss: 9.1719 - accuracy: 0.3772 No face\n",
      " No face\n",
      " 88/106 [=======================>......] - ETA: 3:03 - loss: 9.1701 - accuracy: 0.3771 No face\n",
      " 94/106 [=========================>....] - ETA: 2:01 - loss: 9.1498 - accuracy: 0.3761 No face\n",
      " 96/106 [==========================>...] - ETA: 1:40 - loss: 9.1702 - accuracy: 0.3752 No face\n",
      " 97/106 [==========================>...] - ETA: 1:30 - loss: 9.1840 - accuracy: 0.3759 No face\n",
      "100/106 [===========================>..] - ETA: 1:00 - loss: 9.2334 - accuracy: 0.3735 No face\n",
      "103/106 [============================>.] - ETA: 30s - loss: 9.2884 - accuracy: 0.3702 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.3470 - accuracy: 0.3696  No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1356s 13s/step - loss: 9.3470 - accuracy: 0.3696 - val_loss: 11.6145 - val_accuracy: 0.2760 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "  6/106 [>.............................] - ETA: 11:41 - loss: 11.2364 - accuracy: 0.2883 No face\n",
      "  8/106 [=>............................] - ETA: 12:44 - loss: 11.2778 - accuracy: 0.3142 No face\n",
      "  9/106 [=>............................] - ETA: 12:55 - loss: 11.2745 - accuracy: 0.3152 No face\n",
      " No face\n",
      " 10/106 [=>............................] - ETA: 13:20 - loss: 11.3494 - accuracy: 0.3171 No face\n",
      " 16/106 [===>..........................] - ETA: 13:47 - loss: 11.3103 - accuracy: 0.3033 No face\n",
      " 23/106 [=====>........................] - ETA: 13:06 - loss: 11.0374 - accuracy: 0.3252 No face\n",
      " 25/106 [======>.......................] - ETA: 12:53 - loss: 10.9129 - accuracy: 0.3259 No face\n",
      " 27/106 [======>.......................] - ETA: 12:39 - loss: 10.8140 - accuracy: 0.3289 No face\n",
      " 30/106 [=======>......................] - ETA: 12:14 - loss: 10.6519 - accuracy: 0.3395 No face\n",
      " 34/106 [========>.....................] - ETA: 11:39 - loss: 10.4609 - accuracy: 0.3432 No face\n",
      " 37/106 [=========>....................] - ETA: 11:15 - loss: 10.3468 - accuracy: 0.3418 No face\n",
      " 38/106 [=========>....................] - ETA: 11:07 - loss: 10.3145 - accuracy: 0.3438 No face\n",
      " 40/106 [==========>...................] - ETA: 10:50 - loss: 10.2530 - accuracy: 0.3417 No face\n",
      " 41/106 [==========>...................] - ETA: 10:41 - loss: 10.2267 - accuracy: 0.3412 No face\n",
      " 42/106 [==========>...................] - ETA: 10:33 - loss: 10.2016 - accuracy: 0.3408 No face\n",
      " 43/106 [===========>..................] - ETA: 10:24 - loss: 10.1869 - accuracy: 0.3418 No face\n",
      " 51/106 [=============>................] - ETA: 9:08 - loss: 10.1359 - accuracy: 0.3424 No face\n",
      " 52/106 [=============>................] - ETA: 8:59 - loss: 10.1317 - accuracy: 0.3426 No face\n",
      " 53/106 [==============>...............] - ETA: 8:49 - loss: 10.1336 - accuracy: 0.3404 No face\n",
      " 55/106 [==============>...............] - ETA: 8:30 - loss: 10.1135 - accuracy: 0.3437 No face\n",
      " No face\n",
      " 56/106 [==============>...............] - ETA: 8:20 - loss: 10.1081 - accuracy: 0.3429 No face\n",
      " No face\n",
      " 61/106 [================>.............] - ETA: 7:31 - loss: 10.0602 - accuracy: 0.3439 No face\n",
      " No face\n",
      " 63/106 [================>.............] - ETA: 7:12 - loss: 10.0308 - accuracy: 0.3468 No face\n",
      " 65/106 [=================>............] - ETA: 6:52 - loss: 10.0043 - accuracy: 0.3488 No face\n",
      " No face\n",
      " 71/106 [===================>..........] - ETA: 5:53 - loss: 9.9511 - accuracy: 0.3505 No face\n",
      " 76/106 [====================>.........] - ETA: 5:03 - loss: 9.8864 - accuracy: 0.3565 No face\n",
      " 77/106 [====================>.........] - ETA: 4:53 - loss: 9.8691 - accuracy: 0.3557 No face\n",
      " 82/106 [======================>.......] - ETA: 4:02 - loss: 9.7609 - accuracy: 0.3578 No face\n",
      " 84/106 [======================>.......] - ETA: 3:42 - loss: 9.7047 - accuracy: 0.3591 No face\n",
      " No face\n",
      " 90/106 [========================>.....] - ETA: 2:42 - loss: 9.5775 - accuracy: 0.3619 No face\n",
      " 91/106 [========================>.....] - ETA: 2:32 - loss: 9.5665 - accuracy: 0.3618 No face\n",
      " 95/106 [=========================>....] - ETA: 1:51 - loss: 9.5744 - accuracy: 0.3598 No face\n",
      " No face\n",
      " 96/106 [==========================>...] - ETA: 1:41 - loss: 9.5818 - accuracy: 0.3592 No face\n",
      "100/106 [===========================>..] - ETA: 1:00 - loss: 9.6254 - accuracy: 0.3596 No face\n",
      "102/106 [===========================>..] - ETA: 40s - loss: 9.6449 - accuracy: 0.3594 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.6649 - accuracy: 0.3589  No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1357s 13s/step - loss: 9.6649 - accuracy: 0.3589 - val_loss: 9.8955 - val_accuracy: 0.4050 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "  1/106 [..............................] - ETA: 0s - loss: 9.4359 - accuracy: 0.2812 No face\n",
      "  6/106 [>.............................] - ETA: 13:15 - loss: 9.5257 - accuracy: 0.3560 No face\n",
      " No face\n",
      " No face\n",
      "  9/106 [=>............................] - ETA: 13:48 - loss: 9.5952 - accuracy: 0.3662 No face\n",
      " 14/106 [==>...........................] - ETA: 13:51 - loss: 9.7103 - accuracy: 0.3318 No face\n",
      " 15/106 [===>..........................] - ETA: 13:49 - loss: 9.7761 - accuracy: 0.3397 No face\n",
      " 16/106 [===>..........................] - ETA: 13:44 - loss: 9.8147 - accuracy: 0.3366 No face\n",
      " 21/106 [====>.........................] - ETA: 13:14 - loss: 9.9187 - accuracy: 0.3494 No face\n",
      " 22/106 [=====>........................] - ETA: 13:07 - loss: 9.9166 - accuracy: 0.3525 No face\n",
      " 25/106 [======>.......................] - ETA: 12:43 - loss: 9.9522 - accuracy: 0.3595 No face\n",
      " 32/106 [========>.....................] - ETA: 11:21 - loss: 9.9512 - accuracy: 0.3709 No face\n",
      " No face\n",
      " No face\n",
      " 33/106 [========>.....................] - ETA: 11:13 - loss: 9.9573 - accuracy: 0.3712 No face\n",
      " 36/106 [=========>....................] - ETA: 10:50 - loss: 9.9625 - accuracy: 0.3764 No face\n",
      " 37/106 [=========>....................] - ETA: 10:42 - loss: 9.9593 - accuracy: 0.3731 No face\n",
      " 38/106 [=========>....................] - ETA: 10:33 - loss: 9.9543 - accuracy: 0.3744 No face\n",
      " No face\n",
      " 39/106 [==========>...................] - ETA: 10:24 - loss: 9.9473 - accuracy: 0.3767 No face\n",
      " 42/106 [==========>...................] - ETA: 9:58 - loss: 9.8946 - accuracy: 0.3792  No face\n",
      " 44/106 [===========>..................] - ETA: 9:41 - loss: 9.8747 - accuracy: 0.3748 No face\n",
      " 47/106 [============>.................] - ETA: 9:13 - loss: 9.8193 - accuracy: 0.3744 No face\n",
      " 50/106 [=============>................] - ETA: 8:47 - loss: 9.7770 - accuracy: 0.3766 No face\n",
      " No face\n",
      " 51/106 [=============>................] - ETA: 8:39 - loss: 9.7627 - accuracy: 0.3777 No face\n",
      " No face\n",
      " 52/106 [=============>................] - ETA: 8:30 - loss: 9.7451 - accuracy: 0.3769 No face\n",
      " 53/106 [==============>...............] - ETA: 8:20 - loss: 9.7205 - accuracy: 0.3783 No face\n",
      " No face\n",
      " 55/106 [==============>...............] - ETA: 8:01 - loss: 9.7054 - accuracy: 0.3786 No face\n",
      " 58/106 [===============>..............] - ETA: 7:34 - loss: 9.6612 - accuracy: 0.3792 No face\n",
      " 60/106 [===============>..............] - ETA: 7:16 - loss: 9.6448 - accuracy: 0.3787 No face\n",
      " 63/106 [================>.............] - ETA: 6:49 - loss: 9.6344 - accuracy: 0.3746 No face\n",
      " 64/106 [=================>............] - ETA: 6:40 - loss: 9.6416 - accuracy: 0.3743 No face\n",
      " 65/106 [=================>............] - ETA: 6:31 - loss: 9.6413 - accuracy: 0.3730 No face\n",
      " 66/106 [=================>............] - ETA: 6:22 - loss: 9.6458 - accuracy: 0.3727 No face\n",
      " 67/106 [=================>............] - ETA: 6:12 - loss: 9.6497 - accuracy: 0.3710 No face\n",
      " 70/106 [==================>...........] - ETA: 5:44 - loss: 9.6609 - accuracy: 0.3686 No face\n",
      " 74/106 [===================>..........] - ETA: 5:06 - loss: 9.6528 - accuracy: 0.3696 No face\n",
      " No face\n",
      " 75/106 [====================>.........] - ETA: 4:57 - loss: 9.6445 - accuracy: 0.3704 No face\n",
      " 77/106 [====================>.........] - ETA: 4:38 - loss: 9.6238 - accuracy: 0.3702 No face\n",
      " 78/106 [=====================>........] - ETA: 4:28 - loss: 9.6127 - accuracy: 0.3696 No face\n",
      " No face\n",
      " 81/106 [=====================>........] - ETA: 4:00 - loss: 9.5703 - accuracy: 0.3697 No face\n",
      " 82/106 [======================>.......] - ETA: 3:50 - loss: 9.5568 - accuracy: 0.3703 No face\n",
      " 84/106 [======================>.......] - ETA: 3:31 - loss: 9.5225 - accuracy: 0.3698 No face\n",
      " 85/106 [=======================>......] - ETA: 3:22 - loss: 9.5070 - accuracy: 0.3700 No face\n",
      " 87/106 [=======================>......] - ETA: 3:02 - loss: 9.4817 - accuracy: 0.3703 No face\n",
      " 90/106 [========================>.....] - ETA: 2:33 - loss: 9.4554 - accuracy: 0.3731 No face\n",
      " 96/106 [==========================>...] - ETA: 1:36 - loss: 9.4376 - accuracy: 0.3713 No face\n",
      " 97/106 [==========================>...] - ETA: 1:26 - loss: 9.4366 - accuracy: 0.3708 No face\n",
      "101/106 [===========================>..] - ETA: 48s - loss: 9.4411 - accuracy: 0.3667 No face\n",
      " No face\n",
      "102/106 [===========================>..] - ETA: 38s - loss: 9.4428 - accuracy: 0.3664 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 9.4529 - accuracy: 0.3686 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "106/106 [==============================] - 1293s 12s/step - loss: 9.4529 - accuracy: 0.3686 - val_loss: 9.8142 - val_accuracy: 0.4229 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "  6/106 [>.............................] - ETA: 13:36 - loss: 9.0990 - accuracy: 0.4531 No face\n",
      "  7/106 [>.............................] - ETA: 13:49 - loss: 8.9350 - accuracy: 0.4395 No face\n",
      " No face\n",
      " 10/106 [=>............................] - ETA: 14:08 - loss: 8.6446 - accuracy: 0.4290 No face\n",
      " 15/106 [===>..........................] - ETA: 13:53 - loss: 8.2190 - accuracy: 0.4055 No face\n",
      " 17/106 [===>..........................] - ETA: 13:42 - loss: 8.0902 - accuracy: 0.4045 No face\n",
      " No face\n",
      " No face\n",
      " 18/106 [====>.........................] - ETA: 13:34 - loss: 8.0276 - accuracy: 0.4102 No face\n",
      " No face\n",
      " 19/106 [====>.........................] - ETA: 13:28 - loss: 7.9730 - accuracy: 0.4130 No face\n",
      " No face\n",
      " 21/106 [====>.........................] - ETA: 13:15 - loss: 7.8513 - accuracy: 0.4197 No face\n",
      " 24/106 [=====>........................] - ETA: 12:55 - loss: 7.7332 - accuracy: 0.4093 No face\n",
      " 27/106 [======>.......................] - ETA: 12:28 - loss: 7.6279 - accuracy: 0.4082 No face\n",
      " 30/106 [=======>......................] - ETA: 12:08 - loss: 7.5425 - accuracy: 0.4106 No face\n",
      " No face\n",
      " 35/106 [========>.....................] - ETA: 11:21 - loss: 7.4662 - accuracy: 0.4034 No face\n",
      " 38/106 [=========>....................] - ETA: 10:54 - loss: 7.4171 - accuracy: 0.4007 No face\n",
      " 43/106 [===========>..................] - ETA: 10:08 - loss: 7.3261 - accuracy: 0.4060 No face\n",
      " 44/106 [===========>..................] - ETA: 9:58 - loss: 7.3109 - accuracy: 0.4049  No face\n",
      " 45/106 [===========>..................] - ETA: 9:49 - loss: 7.2999 - accuracy: 0.4038 No face\n",
      " 48/106 [============>.................] - ETA: 9:21 - loss: 7.2471 - accuracy: 0.4042 No face\n",
      " 49/106 [============>.................] - ETA: 9:11 - loss: 7.2281 - accuracy: 0.4019 No face\n",
      " 50/106 [=============>................] - ETA: 9:02 - loss: 7.2120 - accuracy: 0.4036 No face\n",
      " 51/106 [=============>................] - ETA: 8:53 - loss: 7.1945 - accuracy: 0.4039 No face\n",
      " 52/106 [=============>................] - ETA: 8:43 - loss: 7.1845 - accuracy: 0.4048 No face\n",
      " No face\n",
      " 53/106 [==============>...............] - ETA: 8:34 - loss: 7.1709 - accuracy: 0.4035 No face\n",
      " 59/106 [===============>..............] - ETA: 7:36 - loss: 7.0862 - accuracy: 0.4029 No face\n",
      " 60/106 [===============>..............] - ETA: 7:27 - loss: 7.0702 - accuracy: 0.4042 No face\n",
      " 64/106 [=================>............] - ETA: 6:48 - loss: 7.0057 - accuracy: 0.4021 No face\n",
      " 66/106 [=================>............] - ETA: 6:29 - loss: 6.9684 - accuracy: 0.4014 No face\n",
      " 67/106 [=================>............] - ETA: 6:19 - loss: 6.9563 - accuracy: 0.4017 No face\n",
      " 71/106 [===================>..........] - ETA: 5:41 - loss: 6.8946 - accuracy: 0.4008 No face\n",
      " 73/106 [===================>..........] - ETA: 5:21 - loss: 6.8797 - accuracy: 0.4003 No face\n",
      " 74/106 [===================>..........] - ETA: 5:12 - loss: 6.8678 - accuracy: 0.3975 No face\n",
      " 75/106 [====================>.........] - ETA: 5:02 - loss: 6.8656 - accuracy: 0.3974 No face\n",
      " 78/106 [=====================>........] - ETA: 4:30 - loss: 6.8400 - accuracy: 0.3998 No face\n",
      " 82/106 [======================>.......] - ETA: 3:51 - loss: 6.8364 - accuracy: 0.3991 No face\n",
      " No face\n",
      " 83/106 [======================>.......] - ETA: 3:42 - loss: 6.8377 - accuracy: 0.4002 No face\n",
      " No face\n",
      " 84/106 [======================>.......] - ETA: 3:32 - loss: 6.8391 - accuracy: 0.4006 No face\n",
      " 86/106 [=======================>......] - ETA: 3:13 - loss: 6.8419 - accuracy: 0.3994 No face\n",
      " 90/106 [========================>.....] - ETA: 2:34 - loss: 6.8537 - accuracy: 0.3984 No face\n",
      " 93/106 [=========================>....] - ETA: 2:05 - loss: 6.8613 - accuracy: 0.3981 No face\n",
      " 94/106 [=========================>....] - ETA: 1:56 - loss: 6.8605 - accuracy: 0.3984 No face\n",
      " 96/106 [==========================>...] - ETA: 1:36 - loss: 6.8626 - accuracy: 0.3987 No face\n",
      " 97/106 [==========================>...] - ETA: 1:27 - loss: 6.8611 - accuracy: 0.3989 No face\n",
      " 98/106 [==========================>...] - ETA: 1:17 - loss: 6.8612 - accuracy: 0.3974 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.8602 - accuracy: 0.3964 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1300s 12s/step - loss: 6.8602 - accuracy: 0.3964 - val_loss: 6.7170 - val_accuracy: 0.4600 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "  2/106 [..............................] - ETA: 8:11 - loss: 6.3728 - accuracy: 0.5000 No face\n",
      " No face\n",
      "  3/106 [..............................] - ETA: 10:51 - loss: 6.4569 - accuracy: 0.4468 No face\n",
      " No face\n",
      "  4/106 [>.............................] - ETA: 12:26 - loss: 6.4856 - accuracy: 0.4194 No face\n",
      "  5/106 [>.............................] - ETA: 13:06 - loss: 6.4095 - accuracy: 0.4387 No face\n",
      "  6/106 [>.............................] - ETA: 13:34 - loss: 6.3683 - accuracy: 0.4516 No face\n",
      " 12/106 [==>...........................] - ETA: 13:53 - loss: 6.3801 - accuracy: 0.4668 No face\n",
      " 14/106 [==>...........................] - ETA: 13:42 - loss: 6.3399 - accuracy: 0.4523 No face\n",
      " 15/106 [===>..........................] - ETA: 13:37 - loss: 6.3205 - accuracy: 0.4586 No face\n",
      " 16/106 [===>..........................] - ETA: 13:34 - loss: 6.3075 - accuracy: 0.4602 No face\n",
      " 19/106 [====>.........................] - ETA: 13:17 - loss: 6.2447 - accuracy: 0.4523 No face\n",
      " 22/106 [=====>........................] - ETA: 12:55 - loss: 6.1639 - accuracy: 0.4393 No face\n",
      " 23/106 [=====>........................] - ETA: 12:49 - loss: 6.1641 - accuracy: 0.4357 No face\n",
      " 25/106 [======>.......................] - ETA: 12:32 - loss: 6.1048 - accuracy: 0.4338 No face\n",
      " No face\n",
      " 27/106 [======>.......................] - ETA: 12:16 - loss: 6.0616 - accuracy: 0.4269 No face\n",
      " 28/106 [======>.......................] - ETA: 12:08 - loss: 6.0209 - accuracy: 0.4255 No face\n",
      " 31/106 [=======>......................] - ETA: 11:44 - loss: 5.9435 - accuracy: 0.4220 No face\n",
      " 32/106 [========>.....................] - ETA: 11:35 - loss: 5.9320 - accuracy: 0.4249 No face\n",
      " No face\n",
      " 36/106 [=========>....................] - ETA: 11:01 - loss: 5.8952 - accuracy: 0.4191 No face\n",
      " 38/106 [=========>....................] - ETA: 10:44 - loss: 5.8932 - accuracy: 0.4204 No face\n",
      " 39/106 [==========>...................] - ETA: 10:35 - loss: 5.8852 - accuracy: 0.4163 No face\n",
      " No face\n",
      " 41/106 [==========>...................] - ETA: 10:17 - loss: 5.8878 - accuracy: 0.4103 No face\n",
      " 46/106 [============>.................] - ETA: 9:29 - loss: 5.9140 - accuracy: 0.4039 No face\n",
      " 47/106 [============>.................] - ETA: 9:19 - loss: 5.9216 - accuracy: 0.4042 No face\n",
      " 49/106 [============>.................] - ETA: 9:01 - loss: 5.9353 - accuracy: 0.4026 No face\n",
      " No face\n",
      " No face\n",
      " 50/106 [=============>................] - ETA: 8:51 - loss: 5.9451 - accuracy: 0.4028 No face\n",
      " 64/106 [=================>............] - ETA: 6:34 - loss: 6.0751 - accuracy: 0.4046 No face\n",
      " 72/106 [===================>..........] - ETA: 5:19 - loss: 6.1296 - accuracy: 0.3988 No face\n",
      " 73/106 [===================>..........] - ETA: 5:09 - loss: 6.1356 - accuracy: 0.3999 No face\n",
      " 77/106 [====================>.........] - ETA: 4:32 - loss: 6.1680 - accuracy: 0.4008 No face\n",
      " 79/106 [=====================>........] - ETA: 4:13 - loss: 6.1737 - accuracy: 0.4003 No face\n",
      " 81/106 [=====================>........] - ETA: 3:55 - loss: 6.1829 - accuracy: 0.3979 No face\n",
      " 83/106 [======================>.......] - ETA: 3:36 - loss: 6.1914 - accuracy: 0.3982 No face\n",
      " 85/106 [=======================>......] - ETA: 3:17 - loss: 6.2013 - accuracy: 0.3971 No face\n",
      " 86/106 [=======================>......] - ETA: 3:08 - loss: 6.2021 - accuracy: 0.3977 No face\n",
      " 87/106 [=======================>......] - ETA: 2:59 - loss: 6.2034 - accuracy: 0.3983 No face\n",
      " 88/106 [=======================>......] - ETA: 2:49 - loss: 6.2074 - accuracy: 0.3989 No face\n",
      " 89/106 [========================>.....] - ETA: 2:40 - loss: 6.2087 - accuracy: 0.3984 No face\n",
      " 91/106 [========================>.....] - ETA: 2:21 - loss: 6.2154 - accuracy: 0.3994 No face\n",
      " 94/106 [=========================>....] - ETA: 1:53 - loss: 6.2223 - accuracy: 0.3984 No face\n",
      " No face\n",
      " No face\n",
      " 97/106 [==========================>...] - ETA: 1:25 - loss: 6.2294 - accuracy: 0.3984 No face\n",
      "100/106 [===========================>..] - ETA: 56s - loss: 6.2346 - accuracy: 0.3949  No face\n",
      "101/106 [===========================>..] - ETA: 47s - loss: 6.2356 - accuracy: 0.3961 No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 6.2360 - accuracy: 0.3993 No face\n",
      " No face\n",
      "105/106 [============================>.] - ETA: 9s - loss: 6.2342 - accuracy: 0.3993  No face\n",
      " No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.2333 - accuracy: 0.3984 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1270s 12s/step - loss: 6.2333 - accuracy: 0.3984 - val_loss: 6.1305 - val_accuracy: 0.4946 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "  4/106 [>.............................] - ETA: 12:28 - loss: 6.1297 - accuracy: 0.4141 No face\n",
      "  5/106 [>.............................] - ETA: 13:06 - loss: 6.0844 - accuracy: 0.4403 No face\n",
      "  6/106 [>.............................] - ETA: 13:24 - loss: 6.0717 - accuracy: 0.4421 No face\n",
      " 11/106 [==>...........................] - ETA: 13:57 - loss: 5.9473 - accuracy: 0.4269 No face\n",
      " 15/106 [===>..........................] - ETA: 13:46 - loss: 5.8946 - accuracy: 0.4118 No face\n",
      " No face\n",
      " 17/106 [===>..........................] - ETA: 13:38 - loss: 5.8371 - accuracy: 0.4089 No face\n",
      " No face\n",
      " 18/106 [====>.........................] - ETA: 13:32 - loss: 5.8490 - accuracy: 0.4032 No face\n",
      " 28/106 [======>.......................] - ETA: 12:15 - loss: 5.9508 - accuracy: 0.4205 No face\n",
      " 30/106 [=======>......................] - ETA: 11:57 - loss: 5.9738 - accuracy: 0.4200 No face\n",
      " No face\n",
      " 32/106 [========>.....................] - ETA: 11:38 - loss: 5.9809 - accuracy: 0.4140 No face\n",
      " 36/106 [=========>....................] - ETA: 11:03 - loss: 5.9784 - accuracy: 0.4170 No face\n",
      " 38/106 [=========>....................] - ETA: 10:44 - loss: 5.9779 - accuracy: 0.4118 No face\n",
      " 40/106 [==========>...................] - ETA: 10:25 - loss: 5.9696 - accuracy: 0.4134 No face\n",
      " No face\n",
      " 42/106 [==========>...................] - ETA: 10:07 - loss: 5.9590 - accuracy: 0.4182 No face\n",
      " No face\n",
      " 44/106 [===========>..................] - ETA: 9:50 - loss: 5.9479 - accuracy: 0.4168 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      " 45/106 [===========>..................] - ETA: 9:40 - loss: 5.9462 - accuracy: 0.4178 No face\n",
      " 47/106 [============>.................] - ETA: 9:22 - loss: 5.9518 - accuracy: 0.4108 No face\n",
      " 50/106 [=============>................] - ETA: 8:54 - loss: 5.9631 - accuracy: 0.4076 No face\n",
      " 52/106 [=============>................] - ETA: 8:35 - loss: 5.9591 - accuracy: 0.4060 No face\n",
      " 58/106 [===============>..............] - ETA: 7:40 - loss: 5.9649 - accuracy: 0.3969 No face\n",
      " 59/106 [===============>..............] - ETA: 7:31 - loss: 5.9666 - accuracy: 0.3962 No face\n",
      " 61/106 [================>.............] - ETA: 7:12 - loss: 5.9643 - accuracy: 0.3973 No face\n",
      " 76/106 [====================>.........] - ETA: 4:45 - loss: 6.0640 - accuracy: 0.3860 No face\n",
      " 78/106 [=====================>........] - ETA: 4:26 - loss: 6.0980 - accuracy: 0.3888 No face\n",
      " No face\n",
      " No face\n",
      " 81/106 [=====================>........] - ETA: 3:57 - loss: 6.1482 - accuracy: 0.3895 No face\n",
      " 84/106 [======================>.......] - ETA: 3:29 - loss: 6.1843 - accuracy: 0.3883 No face\n",
      " No face\n",
      " 86/106 [=======================>......] - ETA: 3:10 - loss: 6.2014 - accuracy: 0.3902 No face\n",
      " 89/106 [========================>.....] - ETA: 2:41 - loss: 6.2279 - accuracy: 0.3891 No face\n",
      " 91/106 [========================>.....] - ETA: 2:22 - loss: 6.2398 - accuracy: 0.3903 No face\n",
      " 96/106 [==========================>...] - ETA: 1:35 - loss: 6.2456 - accuracy: 0.3916 No face\n",
      " No face\n",
      " 98/106 [==========================>...] - ETA: 1:16 - loss: 6.2538 - accuracy: 0.3902 No face\n",
      "104/106 [============================>.] - ETA: 19s - loss: 6.2497 - accuracy: 0.3885 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.2503 - accuracy: 0.3893 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1265s 12s/step - loss: 6.2503 - accuracy: 0.3893 - val_loss: 6.0259 - val_accuracy: 0.4313 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "  6/106 [>.............................] - ETA: 12:31 - loss: 6.0490 - accuracy: 0.4427 No face\n",
      " 11/106 [==>...........................] - ETA: 13:22 - loss: 6.1121 - accuracy: 0.4274 No face\n",
      " 12/106 [==>...........................] - ETA: 13:22 - loss: 6.1184 - accuracy: 0.4267 No face\n",
      " 17/106 [===>..........................] - ETA: 13:00 - loss: 6.1518 - accuracy: 0.4270 No face\n",
      " 18/106 [====>.........................] - ETA: 12:55 - loss: 6.1631 - accuracy: 0.4318 No face\n",
      " 20/106 [====>.........................] - ETA: 12:43 - loss: 6.1692 - accuracy: 0.4315 No face\n",
      " No face\n",
      " 21/106 [====>.........................] - ETA: 12:38 - loss: 6.1789 - accuracy: 0.4301 No face\n",
      " 26/106 [======>.......................] - ETA: 12:00 - loss: 6.2287 - accuracy: 0.4284 No face\n",
      " 27/106 [======>.......................] - ETA: 11:54 - loss: 6.2350 - accuracy: 0.4269 No face\n",
      " 28/106 [======>.......................] - ETA: 11:47 - loss: 6.2338 - accuracy: 0.4300 No face\n",
      " No face\n",
      " 31/106 [=======>......................] - ETA: 11:22 - loss: 6.2527 - accuracy: 0.4194 No face\n",
      " 34/106 [========>.....................] - ETA: 10:57 - loss: 6.2890 - accuracy: 0.4084 No face\n",
      " 35/106 [========>.....................] - ETA: 10:47 - loss: 6.2889 - accuracy: 0.4114 No face\n",
      " No face\n",
      " 36/106 [=========>....................] - ETA: 10:39 - loss: 6.2924 - accuracy: 0.4111 No face\n",
      " 37/106 [=========>....................] - ETA: 10:29 - loss: 6.2996 - accuracy: 0.4130 No face\n",
      " No face\n",
      " 38/106 [=========>....................] - ETA: 10:19 - loss: 6.2966 - accuracy: 0.4119 No face\n",
      " 39/106 [==========>...................] - ETA: 10:11 - loss: 6.2929 - accuracy: 0.4121 No face\n",
      " 41/106 [==========>...................] - ETA: 9:55 - loss: 6.2975 - accuracy: 0.4167  No face\n",
      " 42/106 [==========>...................] - ETA: 9:46 - loss: 6.2906 - accuracy: 0.4175 No face\n",
      " 43/106 [===========>..................] - ETA: 9:38 - loss: 6.2835 - accuracy: 0.4191 No face\n",
      " 44/106 [===========>..................] - ETA: 9:28 - loss: 6.2821 - accuracy: 0.4198 No face\n",
      " 46/106 [============>.................] - ETA: 9:09 - loss: 6.2637 - accuracy: 0.4195 No face\n",
      " 47/106 [============>.................] - ETA: 9:01 - loss: 6.2551 - accuracy: 0.4181 No face\n",
      " 48/106 [============>.................] - ETA: 8:52 - loss: 6.2382 - accuracy: 0.4201 No face\n",
      " 49/106 [============>.................] - ETA: 8:43 - loss: 6.2269 - accuracy: 0.4162 No face\n",
      " 51/106 [=============>................] - ETA: 8:25 - loss: 6.2074 - accuracy: 0.4180 No face\n",
      " 52/106 [=============>................] - ETA: 8:16 - loss: 6.1964 - accuracy: 0.4174 No face\n",
      " No face\n",
      " 53/106 [==============>...............] - ETA: 8:07 - loss: 6.1934 - accuracy: 0.4183 No face\n",
      " 57/106 [===============>..............] - ETA: 7:30 - loss: 6.1892 - accuracy: 0.4154 No face\n",
      " 61/106 [================>.............] - ETA: 6:47 - loss: 6.1849 - accuracy: 0.4134 No face\n",
      " 63/106 [================>.............] - ETA: 6:30 - loss: 6.1892 - accuracy: 0.4139 No face\n",
      " 64/106 [=================>............] - ETA: 6:21 - loss: 6.1895 - accuracy: 0.4125 No face\n",
      " 65/106 [=================>............] - ETA: 6:12 - loss: 6.1929 - accuracy: 0.4121 No face\n",
      " No face\n",
      " 69/106 [==================>...........] - ETA: 5:37 - loss: 6.2219 - accuracy: 0.4131 No face\n",
      " 71/106 [===================>..........] - ETA: 5:19 - loss: 6.2308 - accuracy: 0.4131 No face\n",
      " 72/106 [===================>..........] - ETA: 5:10 - loss: 6.2383 - accuracy: 0.4132 No face\n",
      " 75/106 [====================>.........] - ETA: 4:43 - loss: 6.2615 - accuracy: 0.4143 No face\n",
      " No face\n",
      " 76/106 [====================>.........] - ETA: 4:34 - loss: 6.2678 - accuracy: 0.4120 No face\n",
      " 78/106 [=====================>........] - ETA: 4:16 - loss: 6.2738 - accuracy: 0.4108 No face\n",
      " 79/106 [=====================>........] - ETA: 4:06 - loss: 6.2749 - accuracy: 0.4109 No face\n",
      " 82/106 [======================>.......] - ETA: 3:39 - loss: 6.2887 - accuracy: 0.4097 No face\n",
      " 83/106 [======================>.......] - ETA: 3:30 - loss: 6.2888 - accuracy: 0.4087 No face\n",
      " 84/106 [======================>.......] - ETA: 3:21 - loss: 6.2937 - accuracy: 0.4088 No face\n",
      " 87/106 [=======================>......] - ETA: 2:53 - loss: 6.2989 - accuracy: 0.4085 No face\n",
      " 88/106 [=======================>......] - ETA: 2:44 - loss: 6.3006 - accuracy: 0.4061 No face\n",
      " 90/106 [========================>.....] - ETA: 2:26 - loss: 6.3094 - accuracy: 0.4044 No face\n",
      "100/106 [===========================>..] - ETA: 55s - loss: 6.3239 - accuracy: 0.3999  No face\n",
      "103/106 [============================>.] - ETA: 27s - loss: 6.3208 - accuracy: 0.3999 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.3117 - accuracy: 0.3981 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1231s 12s/step - loss: 6.3117 - accuracy: 0.3981 - val_loss: 5.9960 - val_accuracy: 0.4982 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "  2/106 [..............................] - ETA: 8:22 - loss: 6.0099 - accuracy: 0.3906 No face\n",
      "  3/106 [..............................] - ETA: 10:32 - loss: 6.0139 - accuracy: 0.4105 No face\n",
      "  7/106 [>.............................] - ETA: 13:24 - loss: 5.9889 - accuracy: 0.4505 No face\n",
      " 10/106 [=>............................] - ETA: 13:42 - loss: 5.9065 - accuracy: 0.4448 No face\n",
      " 11/106 [==>...........................] - ETA: 13:34 - loss: 5.9373 - accuracy: 0.4397 No face\n",
      " 12/106 [==>...........................] - ETA: 13:26 - loss: 5.9665 - accuracy: 0.4274 No face\n",
      " No face\n",
      " 16/106 [===>..........................] - ETA: 13:08 - loss: 6.0484 - accuracy: 0.4238 No face\n",
      " 19/106 [====>.........................] - ETA: 12:54 - loss: 6.1440 - accuracy: 0.4283 No face\n",
      " 20/106 [====>.........................] - ETA: 12:46 - loss: 6.1625 - accuracy: 0.4247 No face\n",
      " No face\n",
      " 25/106 [======>.......................] - ETA: 11:45 - loss: 6.2086 - accuracy: 0.4197 No face\n",
      " 29/106 [=======>......................] - ETA: 11:17 - loss: 6.2888 - accuracy: 0.4081 No face\n",
      " 30/106 [=======>......................] - ETA: 11:09 - loss: 6.2918 - accuracy: 0.4085 No face\n",
      " 32/106 [========>.....................] - ETA: 10:55 - loss: 6.3265 - accuracy: 0.4149 No face\n",
      " 34/106 [========>.....................] - ETA: 10:40 - loss: 6.3615 - accuracy: 0.4071 No face\n",
      " 35/106 [========>.....................] - ETA: 10:32 - loss: 6.3831 - accuracy: 0.4074 No face\n",
      " 40/106 [==========>...................] - ETA: 9:52 - loss: 6.4709 - accuracy: 0.4060 No face\n",
      " 44/106 [===========>..................] - ETA: 9:19 - loss: 6.4848 - accuracy: 0.4056 No face\n",
      " No face\n",
      " No face\n",
      " 45/106 [===========>..................] - ETA: 9:10 - loss: 6.4883 - accuracy: 0.4043 No face\n",
      " 54/106 [==============>...............] - ETA: 7:52 - loss: 6.4830 - accuracy: 0.3983 No face\n",
      " 55/106 [==============>...............] - ETA: 7:43 - loss: 6.4761 - accuracy: 0.3958 No face\n",
      " 56/106 [==============>...............] - ETA: 7:34 - loss: 6.4659 - accuracy: 0.3962 No face\n",
      " 57/106 [===============>..............] - ETA: 7:25 - loss: 6.4743 - accuracy: 0.3955 No face\n",
      " 58/106 [===============>..............] - ETA: 7:16 - loss: 6.4655 - accuracy: 0.3948 No face\n",
      " 59/106 [===============>..............] - ETA: 7:07 - loss: 6.4589 - accuracy: 0.3947 No face\n",
      " No face\n",
      " 63/106 [================>.............] - ETA: 6:32 - loss: 6.4171 - accuracy: 0.3973 No face\n",
      " 65/106 [=================>............] - ETA: 6:14 - loss: 6.4016 - accuracy: 0.3973 No face\n",
      " 68/106 [==================>...........] - ETA: 5:47 - loss: 6.3666 - accuracy: 0.4026 No face\n",
      " No face\n",
      " 69/106 [==================>...........] - ETA: 5:38 - loss: 6.3535 - accuracy: 0.4035 No face\n",
      " 72/106 [===================>..........] - ETA: 5:11 - loss: 6.3140 - accuracy: 0.4052 No face\n",
      " No face\n",
      " 80/106 [=====================>........] - ETA: 3:58 - loss: 6.2625 - accuracy: 0.4028 No face\n",
      " 81/106 [=====================>........] - ETA: 3:49 - loss: 6.2555 - accuracy: 0.4026 No face\n",
      " 84/106 [======================>.......] - ETA: 3:22 - loss: 6.2421 - accuracy: 0.4037 No face\n",
      " 87/106 [=======================>......] - ETA: 2:54 - loss: 6.2393 - accuracy: 0.4017 No face\n",
      " 92/106 [=========================>....] - ETA: 2:08 - loss: 6.2249 - accuracy: 0.4077 No face\n",
      " No face\n",
      " 95/106 [=========================>....] - ETA: 1:41 - loss: 6.2191 - accuracy: 0.4072 No face\n",
      " No face\n",
      " No face\n",
      "102/106 [===========================>..] - ETA: 36s - loss: 6.2191 - accuracy: 0.4100 No face\n",
      "103/106 [============================>.] - ETA: 27s - loss: 6.2156 - accuracy: 0.4114 No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 6.2115 - accuracy: 0.4114 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.2086 - accuracy: 0.4100 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1235s 12s/step - loss: 6.2086 - accuracy: 0.4100 - val_loss: 6.0258 - val_accuracy: 0.4217 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      " No face\n",
      "  2/106 [..............................] - ETA: 7:25 - loss: 6.0124 - accuracy: 0.5397 No face\n",
      "  5/106 [>.............................] - ETA: 12:44 - loss: 5.9527 - accuracy: 0.4873 No face\n",
      "  6/106 [>.............................] - ETA: 13:13 - loss: 5.9499 - accuracy: 0.4709 No face\n",
      "  9/106 [=>............................] - ETA: 13:44 - loss: 5.8288 - accuracy: 0.4437 No face\n",
      " 22/106 [=====>........................] - ETA: 12:48 - loss: 5.8817 - accuracy: 0.4034 No face\n",
      " 26/106 [======>.......................] - ETA: 12:12 - loss: 5.9121 - accuracy: 0.3862 No face\n",
      " 27/106 [======>.......................] - ETA: 12:05 - loss: 5.9189 - accuracy: 0.3886 No face\n",
      " 31/106 [=======>......................] - ETA: 11:32 - loss: 5.9209 - accuracy: 0.3770 No face\n",
      " 33/106 [========>.....................] - ETA: 11:13 - loss: 5.9199 - accuracy: 0.3792 No face\n",
      " No face\n",
      " 36/106 [=========>....................] - ETA: 10:46 - loss: 5.9264 - accuracy: 0.3699 No face\n",
      " No face\n",
      " 38/106 [=========>....................] - ETA: 10:28 - loss: 5.9297 - accuracy: 0.3741 No face\n",
      " 41/106 [==========>...................] - ETA: 10:02 - loss: 5.9587 - accuracy: 0.3713 No face\n",
      " No face\n",
      " No face\n",
      " 43/106 [===========>..................] - ETA: 9:45 - loss: 5.9773 - accuracy: 0.3694 No face\n",
      " 44/106 [===========>..................] - ETA: 9:36 - loss: 5.9773 - accuracy: 0.3676 No face\n",
      " 45/106 [===========>..................] - ETA: 9:26 - loss: 5.9806 - accuracy: 0.3659 No face\n",
      " No face\n",
      " 46/106 [============>.................] - ETA: 9:18 - loss: 5.9990 - accuracy: 0.3653 No face\n",
      " 50/106 [=============>................] - ETA: 8:41 - loss: 6.0124 - accuracy: 0.3631 No face\n",
      " No face\n",
      " 51/106 [=============>................] - ETA: 8:31 - loss: 6.0127 - accuracy: 0.3626 No face\n",
      " 52/106 [=============>................] - ETA: 8:23 - loss: 6.0147 - accuracy: 0.3649 No face\n",
      " 55/106 [==============>...............] - ETA: 7:56 - loss: 6.0183 - accuracy: 0.3691 No face\n",
      " 58/106 [===============>..............] - ETA: 7:27 - loss: 6.0421 - accuracy: 0.3685 No face\n",
      " 60/106 [===============>..............] - ETA: 7:08 - loss: 6.0506 - accuracy: 0.3695 No face\n",
      " No face\n",
      " 61/106 [================>.............] - ETA: 6:59 - loss: 6.0661 - accuracy: 0.3694 No face\n",
      " 64/106 [=================>............] - ETA: 6:31 - loss: 6.0859 - accuracy: 0.3704 No face\n",
      " 65/106 [=================>............] - ETA: 6:22 - loss: 6.0887 - accuracy: 0.3721 No face\n",
      " No face\n",
      " No face\n",
      " 70/106 [==================>...........] - ETA: 5:36 - loss: 6.1047 - accuracy: 0.3769 No face\n",
      " 71/106 [===================>..........] - ETA: 5:27 - loss: 6.1058 - accuracy: 0.3770 No face\n",
      " 72/106 [===================>..........] - ETA: 5:17 - loss: 6.1042 - accuracy: 0.3776 No face\n",
      " 73/106 [===================>..........] - ETA: 5:08 - loss: 6.1042 - accuracy: 0.3799 No face\n",
      " 78/106 [=====================>........] - ETA: 4:22 - loss: 6.1183 - accuracy: 0.3810 No face\n",
      " No face\n",
      " 81/106 [=====================>........] - ETA: 3:54 - loss: 6.1162 - accuracy: 0.3830 No face\n",
      " No face\n",
      " 84/106 [======================>.......] - ETA: 3:26 - loss: 6.1167 - accuracy: 0.3864 No face\n",
      " 87/106 [=======================>......] - ETA: 2:56 - loss: 6.1115 - accuracy: 0.3873 No face\n",
      " No face\n",
      " No face\n",
      " 90/106 [========================>.....] - ETA: 2:28 - loss: 6.1061 - accuracy: 0.3873 No face\n",
      " 93/106 [=========================>....] - ETA: 2:00 - loss: 6.1172 - accuracy: 0.3888 No face\n",
      " 94/106 [=========================>....] - ETA: 1:51 - loss: 6.1182 - accuracy: 0.3870 No face\n",
      " No face\n",
      " 95/106 [=========================>....] - ETA: 1:42 - loss: 6.1170 - accuracy: 0.3875 No face\n",
      " 96/106 [==========================>...] - ETA: 1:32 - loss: 6.1249 - accuracy: 0.3878 No face\n",
      "105/106 [============================>.] - ETA: 9s - loss: 6.1225 - accuracy: 0.3899  No face\n",
      " No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.1205 - accuracy: 0.3918 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1248s 12s/step - loss: 6.1205 - accuracy: 0.3918 - val_loss: 5.7786 - val_accuracy: 0.4815 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "  1/106 [..............................] - ETA: 0s - loss: 5.6271 - accuracy: 0.5938 No face\n",
      "  2/106 [..............................] - ETA: 8:18 - loss: 5.4967 - accuracy: 0.5079 No face\n",
      "  4/106 [>.............................] - ETA: 11:48 - loss: 5.5439 - accuracy: 0.5238 No face\n",
      "  5/106 [>.............................] - ETA: 12:39 - loss: 5.4820 - accuracy: 0.5096 No face\n",
      "  6/106 [>.............................] - ETA: 12:59 - loss: 5.4758 - accuracy: 0.4894 No face\n",
      " 10/106 [=>............................] - ETA: 13:40 - loss: 5.4079 - accuracy: 0.4825 No face\n",
      " 11/106 [==>...........................] - ETA: 13:42 - loss: 5.4148 - accuracy: 0.4855 No face\n",
      " 16/106 [===>..........................] - ETA: 13:24 - loss: 5.4345 - accuracy: 0.4673 No face\n",
      " No face\n",
      " No face\n",
      " 19/106 [====>.........................] - ETA: 13:07 - loss: 5.4619 - accuracy: 0.4682 No face\n",
      " 23/106 [=====>........................] - ETA: 12:38 - loss: 5.4683 - accuracy: 0.4566 No face\n",
      " 26/106 [======>.......................] - ETA: 12:15 - loss: 5.4933 - accuracy: 0.4659 No face\n",
      " 30/106 [=======>......................] - ETA: 11:41 - loss: 5.5376 - accuracy: 0.4562 No face\n",
      " 31/106 [=======>......................] - ETA: 11:32 - loss: 5.5474 - accuracy: 0.4581 No face\n",
      " 40/106 [==========>...................] - ETA: 10:17 - loss: 5.7101 - accuracy: 0.4458 No face\n",
      " 46/106 [============>.................] - ETA: 9:24 - loss: 5.7448 - accuracy: 0.4368 No face\n",
      " 49/106 [============>.................] - ETA: 8:57 - loss: 5.7624 - accuracy: 0.4391 No face\n",
      " 51/106 [=============>................] - ETA: 8:39 - loss: 5.7765 - accuracy: 0.4399 No face\n",
      " 52/106 [=============>................] - ETA: 8:29 - loss: 5.7882 - accuracy: 0.4389 No face\n",
      " No face\n",
      " 53/106 [==============>...............] - ETA: 8:20 - loss: 5.7920 - accuracy: 0.4382 No face\n",
      " 54/106 [==============>...............] - ETA: 8:12 - loss: 5.7949 - accuracy: 0.4390 No face\n",
      " 56/106 [==============>...............] - ETA: 7:53 - loss: 5.8040 - accuracy: 0.4387 No face\n",
      " 57/106 [===============>..............] - ETA: 7:43 - loss: 5.8110 - accuracy: 0.4361 No face\n",
      " 59/106 [===============>..............] - ETA: 7:24 - loss: 5.8234 - accuracy: 0.4353 No face\n",
      " 67/106 [=================>............] - ETA: 6:10 - loss: 5.8297 - accuracy: 0.4348 No face\n",
      " 70/106 [==================>...........] - ETA: 5:41 - loss: 5.8247 - accuracy: 0.4306 No face\n",
      " No face\n",
      " 71/106 [===================>..........] - ETA: 5:32 - loss: 5.8176 - accuracy: 0.4298 No face\n",
      " 73/106 [===================>..........] - ETA: 5:09 - loss: 5.8152 - accuracy: 0.4295 No face\n",
      " 76/106 [====================>.........] - ETA: 4:41 - loss: 5.8130 - accuracy: 0.4304 No face\n",
      " 80/106 [=====================>........] - ETA: 4:04 - loss: 5.8200 - accuracy: 0.4254 No face\n",
      " 81/106 [=====================>........] - ETA: 3:55 - loss: 5.8210 - accuracy: 0.4245 No face\n",
      " No face\n",
      " 84/106 [======================>.......] - ETA: 3:26 - loss: 5.8289 - accuracy: 0.4226 No face\n",
      " 85/106 [=======================>......] - ETA: 3:17 - loss: 5.8279 - accuracy: 0.4237 No face\n",
      " 87/106 [=======================>......] - ETA: 2:59 - loss: 5.8229 - accuracy: 0.4242 No face\n",
      " 97/106 [==========================>...] - ETA: 1:24 - loss: 5.7916 - accuracy: 0.4234 No face\n",
      " 98/106 [==========================>...] - ETA: 1:15 - loss: 5.7872 - accuracy: 0.4224 No face\n",
      "101/106 [===========================>..] - ETA: 47s - loss: 5.7832 - accuracy: 0.4186 No face\n",
      "103/106 [============================>.] - ETA: 28s - loss: 5.7803 - accuracy: 0.4179 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 5.7885 - accuracy: 0.4167 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1259s 12s/step - loss: 5.7885 - accuracy: 0.4167 - val_loss: 5.8784 - val_accuracy: 0.4444 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      " No face\n",
      "  4/106 [>.............................] - ETA: 12:13 - loss: 6.1930 - accuracy: 0.4646 No face\n",
      "  7/106 [>.............................] - ETA: 13:41 - loss: 6.2056 - accuracy: 0.4279 No face\n",
      " No face\n",
      "  8/106 [=>............................] - ETA: 13:46 - loss: 6.1917 - accuracy: 0.4087 No face\n",
      " 11/106 [==>...........................] - ETA: 13:49 - loss: 6.1287 - accuracy: 0.4150 No face\n",
      " 14/106 [==>...........................] - ETA: 13:37 - loss: 6.1107 - accuracy: 0.4118 No face\n",
      " 16/106 [===>..........................] - ETA: 13:26 - loss: 6.0738 - accuracy: 0.4178 No face\n",
      " 18/106 [====>.........................] - ETA: 13:15 - loss: 6.0418 - accuracy: 0.4173 No face\n",
      " 19/106 [====>.........................] - ETA: 13:09 - loss: 5.9990 - accuracy: 0.4207 No face\n",
      " No face\n",
      " 20/106 [====>.........................] - ETA: 13:00 - loss: 5.9873 - accuracy: 0.4229 No face\n",
      " 21/106 [====>.........................] - ETA: 12:53 - loss: 5.9950 - accuracy: 0.4212 No face\n",
      " No face\n",
      " 24/106 [=====>........................] - ETA: 12:30 - loss: 5.9483 - accuracy: 0.4164 No face\n",
      " 27/106 [======>.......................] - ETA: 12:07 - loss: 5.9378 - accuracy: 0.4075 No face\n",
      " 29/106 [=======>......................] - ETA: 11:50 - loss: 5.9099 - accuracy: 0.4090 No face\n",
      " 30/106 [=======>......................] - ETA: 11:40 - loss: 5.8972 - accuracy: 0.4093 No face\n",
      " 31/106 [=======>......................] - ETA: 11:30 - loss: 5.8913 - accuracy: 0.4107 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      " 37/106 [=========>....................] - ETA: 10:24 - loss: 5.9000 - accuracy: 0.4113 No face\n",
      " 38/106 [=========>....................] - ETA: 10:16 - loss: 5.9042 - accuracy: 0.4098 No face\n",
      " 40/106 [==========>...................] - ETA: 10:00 - loss: 5.9225 - accuracy: 0.4099 No face\n",
      " 41/106 [==========>...................] - ETA: 9:51 - loss: 5.9311 - accuracy: 0.4102  No face\n",
      " 42/106 [==========>...................] - ETA: 9:43 - loss: 5.9377 - accuracy: 0.4081 No face\n",
      " No face\n",
      " 44/106 [===========>..................] - ETA: 9:26 - loss: 5.9631 - accuracy: 0.4078 No face\n",
      " 46/106 [============>.................] - ETA: 9:09 - loss: 5.9946 - accuracy: 0.4088 No face\n",
      " 47/106 [============>.................] - ETA: 9:00 - loss: 6.0028 - accuracy: 0.4083 No face\n",
      " No face\n",
      " 48/106 [============>.................] - ETA: 8:52 - loss: 6.0202 - accuracy: 0.4081 No face\n",
      " 49/106 [============>.................] - ETA: 8:43 - loss: 6.0362 - accuracy: 0.4084 No face\n",
      " 53/106 [==============>...............] - ETA: 8:10 - loss: 6.1090 - accuracy: 0.4023 No face\n",
      " 56/106 [==============>...............] - ETA: 7:42 - loss: 6.1466 - accuracy: 0.4028 No face\n",
      " 58/106 [===============>..............] - ETA: 7:25 - loss: 6.1596 - accuracy: 0.4009 No face\n",
      " No face\n",
      " No face\n",
      " 59/106 [===============>..............] - ETA: 7:16 - loss: 6.1654 - accuracy: 0.4011 No face\n",
      " No face\n",
      " 61/106 [================>.............] - ETA: 6:58 - loss: 6.1878 - accuracy: 0.4001 No face\n",
      " 64/106 [=================>............] - ETA: 6:31 - loss: 6.2125 - accuracy: 0.4026 No face\n",
      " 66/106 [=================>............] - ETA: 6:13 - loss: 6.2278 - accuracy: 0.4010 No face\n",
      " No face\n",
      " No face\n",
      " 67/106 [=================>............] - ETA: 6:03 - loss: 6.2344 - accuracy: 0.4002 No face\n",
      " 68/106 [==================>...........] - ETA: 5:55 - loss: 6.2493 - accuracy: 0.3990 No face\n",
      " 71/106 [===================>..........] - ETA: 5:28 - loss: 6.2714 - accuracy: 0.3959 No face\n",
      " 73/106 [===================>..........] - ETA: 5:09 - loss: 6.2887 - accuracy: 0.3964 No face\n",
      " 74/106 [===================>..........] - ETA: 4:59 - loss: 6.3029 - accuracy: 0.3954 No face\n",
      " 83/106 [======================>.......] - ETA: 3:36 - loss: 6.3830 - accuracy: 0.3901 No face\n",
      " 84/106 [======================>.......] - ETA: 3:26 - loss: 6.3881 - accuracy: 0.3901 No face\n",
      " 85/106 [=======================>......] - ETA: 3:17 - loss: 6.3923 - accuracy: 0.3889 No face\n",
      " No face\n",
      " 87/106 [=======================>......] - ETA: 2:58 - loss: 6.3994 - accuracy: 0.3870 No face\n",
      " 88/106 [=======================>......] - ETA: 2:49 - loss: 6.4009 - accuracy: 0.3863 No face\n",
      " 89/106 [========================>.....] - ETA: 2:40 - loss: 6.3983 - accuracy: 0.3860 No face\n",
      " No face\n",
      " 90/106 [========================>.....] - ETA: 2:30 - loss: 6.3943 - accuracy: 0.3861 No face\n",
      " 91/106 [========================>.....] - ETA: 2:21 - loss: 6.3960 - accuracy: 0.3861 No face\n",
      " 92/106 [=========================>....] - ETA: 2:11 - loss: 6.3896 - accuracy: 0.3872 No face\n",
      " 98/106 [==========================>...] - ETA: 1:15 - loss: 6.3529 - accuracy: 0.3915 No face\n",
      "100/106 [===========================>..] - ETA: 56s - loss: 6.3332 - accuracy: 0.3916  No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 6.2928 - accuracy: 0.3926 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 6.2778 - accuracy: 0.3939 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1270s 12s/step - loss: 6.2778 - accuracy: 0.3939 - val_loss: 5.2592 - val_accuracy: 0.4898 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      " No face\n",
      "  3/106 [..............................] - ETA: 10:42 - loss: 5.1068 - accuracy: 0.3789 No face\n",
      "  6/106 [>.............................] - ETA: 13:12 - loss: 5.1515 - accuracy: 0.4368 No face\n",
      "  7/106 [>.............................] - ETA: 13:25 - loss: 5.1685 - accuracy: 0.4344 No face\n",
      "  8/106 [=>............................] - ETA: 13:39 - loss: 5.1515 - accuracy: 0.4524 No face\n",
      " 13/106 [==>...........................] - ETA: 13:40 - loss: 5.1787 - accuracy: 0.4307 No face\n",
      " No face\n",
      " 20/106 [====>.........................] - ETA: 13:07 - loss: 5.2263 - accuracy: 0.4155 No face\n",
      " No face\n",
      " No face\n",
      " 23/106 [=====>........................] - ETA: 12:48 - loss: 5.2525 - accuracy: 0.4118 No face\n",
      " 25/106 [======>.......................] - ETA: 12:35 - loss: 5.2671 - accuracy: 0.4068 No face\n",
      " No face\n",
      " 27/106 [======>.......................] - ETA: 12:20 - loss: 5.2881 - accuracy: 0.3995 No face\n",
      " No face\n",
      " 35/106 [========>.....................] - ETA: 11:13 - loss: 5.3379 - accuracy: 0.3900 No face\n",
      " 38/106 [=========>....................] - ETA: 10:46 - loss: 5.3513 - accuracy: 0.3933 No face\n",
      " 39/106 [==========>...................] - ETA: 10:38 - loss: 5.3479 - accuracy: 0.3932 No face\n",
      " No face\n",
      " 40/106 [==========>...................] - ETA: 10:28 - loss: 5.3444 - accuracy: 0.3981 No face\n",
      " 41/106 [==========>...................] - ETA: 10:19 - loss: 5.3408 - accuracy: 0.3994 No face\n",
      " No face\n",
      " 44/106 [===========>..................] - ETA: 9:52 - loss: 5.3130 - accuracy: 0.3954  No face\n",
      " 48/106 [============>.................] - ETA: 9:13 - loss: 5.2696 - accuracy: 0.4085 No face\n",
      " 53/106 [==============>...............] - ETA: 8:27 - loss: 5.2260 - accuracy: 0.4031 No face\n",
      " 55/106 [==============>...............] - ETA: 8:07 - loss: 5.2163 - accuracy: 0.4046 No face\n",
      " 56/106 [==============>...............] - ETA: 7:58 - loss: 5.2065 - accuracy: 0.4060 No face\n",
      " 57/106 [===============>..............] - ETA: 7:49 - loss: 5.2093 - accuracy: 0.4062 No face\n",
      " No face\n",
      " 66/106 [=================>............] - ETA: 6:24 - loss: 5.2539 - accuracy: 0.4052 No face\n",
      " 70/106 [==================>...........] - ETA: 5:46 - loss: 5.2783 - accuracy: 0.4100 No face\n",
      " 71/106 [===================>..........] - ETA: 5:36 - loss: 5.2833 - accuracy: 0.4119 No face\n",
      " 82/106 [======================>.......] - ETA: 3:49 - loss: 5.3416 - accuracy: 0.4112 No face\n",
      " No face\n",
      " 84/106 [======================>.......] - ETA: 3:30 - loss: 5.3571 - accuracy: 0.4107 No face\n",
      " 86/106 [=======================>......] - ETA: 3:11 - loss: 5.3691 - accuracy: 0.4100 No face\n",
      " 87/106 [=======================>......] - ETA: 3:01 - loss: 5.3748 - accuracy: 0.4097 No face\n",
      " 93/106 [=========================>....] - ETA: 2:04 - loss: 5.4153 - accuracy: 0.4103 No face\n",
      " No face\n",
      "101/106 [===========================>..] - ETA: 47s - loss: 5.4648 - accuracy: 0.4099 No face\n",
      "103/106 [============================>.] - ETA: 28s - loss: 5.4889 - accuracy: 0.4106 No face\n",
      "104/106 [============================>.] - ETA: 19s - loss: 5.4918 - accuracy: 0.4113 No face\n",
      "105/106 [============================>.] - ETA: 9s - loss: 5.4952 - accuracy: 0.4126  No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 5.5078 - accuracy: 0.4111 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1286s 12s/step - loss: 5.5078 - accuracy: 0.4111 - val_loss: 6.3435 - val_accuracy: 0.4432 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "  1/106 [..............................] - ETA: 0s - loss: 6.2903 - accuracy: 0.4688 No face\n",
      " No face\n",
      "  2/106 [..............................] - ETA: 8:08 - loss: 6.2336 - accuracy: 0.5323 No face\n",
      "  4/106 [>.............................] - ETA: 12:12 - loss: 6.3625 - accuracy: 0.4160 No face\n",
      "  5/106 [>.............................] - ETA: 13:07 - loss: 6.3561 - accuracy: 0.4103 No face\n",
      " No face\n",
      "  6/106 [>.............................] - ETA: 13:30 - loss: 6.4871 - accuracy: 0.4086 No face\n",
      "  8/106 [=>............................] - ETA: 13:57 - loss: 6.5698 - accuracy: 0.4137 No face\n",
      " No face\n",
      " 13/106 [==>...........................] - ETA: 12:55 - loss: 6.6048 - accuracy: 0.4101 No face\n",
      " 14/106 [==>...........................] - ETA: 12:59 - loss: 6.6197 - accuracy: 0.4059 No face\n",
      " 16/106 [===>..........................] - ETA: 12:55 - loss: 6.6398 - accuracy: 0.4047 No face\n",
      " 18/106 [====>.........................] - ETA: 12:49 - loss: 6.6305 - accuracy: 0.4112 No face\n",
      " 21/106 [====>.........................] - ETA: 12:35 - loss: 6.6639 - accuracy: 0.4143 No face\n",
      " 23/106 [=====>........................] - ETA: 12:26 - loss: 6.6550 - accuracy: 0.4040 No face\n",
      " 24/106 [=====>........................] - ETA: 12:19 - loss: 6.6445 - accuracy: 0.4116 No face\n",
      " 26/106 [======>.......................] - ETA: 12:07 - loss: 6.6287 - accuracy: 0.4091 No face\n",
      " 28/106 [======>.......................] - ETA: 11:51 - loss: 6.5845 - accuracy: 0.4035 No face\n",
      " 30/106 [=======>......................] - ETA: 11:35 - loss: 6.5420 - accuracy: 0.4042 No face\n",
      " 32/106 [========>.....................] - ETA: 11:23 - loss: 6.4941 - accuracy: 0.4078 No face\n",
      " 36/106 [=========>....................] - ETA: 10:50 - loss: 6.4273 - accuracy: 0.4016 No face\n",
      " No face\n",
      " 38/106 [=========>....................] - ETA: 10:34 - loss: 6.3896 - accuracy: 0.4026 No face\n",
      " No face\n",
      " 42/106 [==========>...................] - ETA: 9:59 - loss: 6.3132 - accuracy: 0.4020  No face\n",
      " 45/106 [===========>..................] - ETA: 9:32 - loss: 6.2639 - accuracy: 0.3983 No face\n",
      " No face\n",
      " 48/106 [============>.................] - ETA: 9:04 - loss: 6.2160 - accuracy: 0.4007 No face\n",
      " 49/106 [============>.................] - ETA: 8:55 - loss: 6.1953 - accuracy: 0.4004 No face\n",
      " 51/106 [=============>................] - ETA: 8:36 - loss: 6.1598 - accuracy: 0.4041 No face\n",
      " 53/106 [==============>...............] - ETA: 8:19 - loss: 6.1325 - accuracy: 0.4044 No face\n",
      " 56/106 [==============>...............] - ETA: 7:52 - loss: 6.1015 - accuracy: 0.4036 No face\n",
      " No face\n",
      " 57/106 [===============>..............] - ETA: 7:43 - loss: 6.0857 - accuracy: 0.4052 No face\n",
      " 58/106 [===============>..............] - ETA: 7:33 - loss: 6.0715 - accuracy: 0.4055 No face\n",
      " 60/106 [===============>..............] - ETA: 7:15 - loss: 6.0560 - accuracy: 0.4036 No face\n",
      " 77/106 [====================>.........] - ETA: 4:36 - loss: 5.9804 - accuracy: 0.4018 No face\n",
      " 82/106 [======================>.......] - ETA: 3:48 - loss: 5.9839 - accuracy: 0.3980 No face\n",
      " 84/106 [======================>.......] - ETA: 3:29 - loss: 5.9803 - accuracy: 0.3960 No face\n",
      " 86/106 [=======================>......] - ETA: 3:10 - loss: 5.9803 - accuracy: 0.3949 No face\n",
      " 87/106 [=======================>......] - ETA: 3:01 - loss: 5.9789 - accuracy: 0.3945 No face\n",
      " 90/106 [========================>.....] - ETA: 2:32 - loss: 5.9745 - accuracy: 0.3940 No face\n",
      " 93/106 [=========================>....] - ETA: 2:04 - loss: 5.9716 - accuracy: 0.3907 No face\n",
      " 96/106 [==========================>...] - ETA: 1:35 - loss: 5.9696 - accuracy: 0.3887 No face\n",
      " 97/106 [==========================>...] - ETA: 1:25 - loss: 5.9678 - accuracy: 0.3887 No face\n",
      " 98/106 [==========================>...] - ETA: 1:16 - loss: 5.9651 - accuracy: 0.3890 No face\n",
      " No face\n",
      " 99/106 [===========================>..] - ETA: 1:06 - loss: 5.9614 - accuracy: 0.3891 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 5.9301 - accuracy: 0.3885 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1280s 12s/step - loss: 5.9301 - accuracy: 0.3885 - val_loss: 5.3705 - val_accuracy: 0.4385 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "  2/106 [..............................] - ETA: 8:07 - loss: 5.4018 - accuracy: 0.4375 No face\n",
      "  3/106 [..............................] - ETA: 11:02 - loss: 5.3235 - accuracy: 0.3684 No face\n",
      "  4/106 [>.............................] - ETA: 12:11 - loss: 5.2571 - accuracy: 0.3810 No face\n",
      "  6/106 [>.............................] - ETA: 13:20 - loss: 5.2862 - accuracy: 0.3862 No face\n",
      " No face\n",
      "  7/106 [>.............................] - ETA: 13:30 - loss: 5.2637 - accuracy: 0.3927 No face\n",
      " 11/106 [==>...........................] - ETA: 13:51 - loss: 5.2762 - accuracy: 0.3844 No face\n",
      " 17/106 [===>..........................] - ETA: 13:30 - loss: 5.3081 - accuracy: 0.3929 No face\n",
      " No face\n",
      " 21/106 [====>.........................] - ETA: 13:05 - loss: 5.3144 - accuracy: 0.3952 No face\n",
      " 24/106 [=====>........................] - ETA: 12:43 - loss: 5.3226 - accuracy: 0.3892 No face\n",
      " 26/106 [======>.......................] - ETA: 12:25 - loss: 5.3299 - accuracy: 0.3861 No face\n",
      " No face\n",
      " No face\n",
      " 28/106 [======>.......................] - ETA: 12:10 - loss: 5.3500 - accuracy: 0.3889 No face\n",
      " 32/106 [========>.....................] - ETA: 11:36 - loss: 5.3797 - accuracy: 0.3865 No face\n",
      " 33/106 [========>.....................] - ETA: 11:27 - loss: 5.3874 - accuracy: 0.3885 No face\n",
      " No face\n",
      " No face\n",
      " 36/106 [=========>....................] - ETA: 10:42 - loss: 5.4245 - accuracy: 0.3904 No face\n",
      " 38/106 [=========>....................] - ETA: 10:25 - loss: 5.4606 - accuracy: 0.3899 No face\n",
      " 39/106 [==========>...................] - ETA: 10:17 - loss: 5.4734 - accuracy: 0.3898 No face\n",
      " 40/106 [==========>...................] - ETA: 10:08 - loss: 5.4906 - accuracy: 0.3906 No face\n",
      " 41/106 [==========>...................] - ETA: 10:00 - loss: 5.4991 - accuracy: 0.3905 No face\n",
      " 43/106 [===========>..................] - ETA: 9:42 - loss: 5.5504 - accuracy: 0.3862 No face\n",
      " 49/106 [============>.................] - ETA: 8:49 - loss: 5.6799 - accuracy: 0.3778 No face\n",
      " No face\n",
      " No face\n",
      " 52/106 [=============>................] - ETA: 8:22 - loss: 5.7356 - accuracy: 0.3740 No face\n",
      " 54/106 [==============>...............] - ETA: 8:05 - loss: 5.7668 - accuracy: 0.3743 No face\n",
      " 58/106 [===============>..............] - ETA: 7:28 - loss: 5.8166 - accuracy: 0.3806 No face\n",
      " No face\n",
      " 61/106 [================>.............] - ETA: 6:59 - loss: 5.8465 - accuracy: 0.3813 No face\n",
      " 64/106 [=================>............] - ETA: 6:32 - loss: 5.8606 - accuracy: 0.3802 No face\n",
      " No face\n",
      " 67/106 [=================>............] - ETA: 6:04 - loss: 5.8768 - accuracy: 0.3822 No face\n",
      " 68/106 [==================>...........] - ETA: 5:54 - loss: 5.8758 - accuracy: 0.3832 No face\n",
      " No face\n",
      " 69/106 [==================>...........] - ETA: 5:45 - loss: 5.8799 - accuracy: 0.3816 No face\n",
      " 70/106 [==================>...........] - ETA: 5:36 - loss: 5.8828 - accuracy: 0.3831 No face\n",
      " 72/106 [===================>..........] - ETA: 5:18 - loss: 5.8841 - accuracy: 0.3839 No face\n",
      " No face\n",
      " 75/106 [====================>.........] - ETA: 4:50 - loss: 5.8707 - accuracy: 0.3869 No face\n",
      " 77/106 [====================>.........] - ETA: 4:31 - loss: 5.8663 - accuracy: 0.3875 No face\n",
      " No face\n",
      " 81/106 [=====================>........] - ETA: 3:54 - loss: 5.8414 - accuracy: 0.3856 No face\n",
      " 83/106 [======================>.......] - ETA: 3:35 - loss: 5.8414 - accuracy: 0.3859 No face\n",
      " 89/106 [========================>.....] - ETA: 2:39 - loss: 5.8169 - accuracy: 0.3856 No face\n",
      " 96/106 [==========================>...] - ETA: 1:34 - loss: 5.7792 - accuracy: 0.3876 No face\n",
      " 98/106 [==========================>...] - ETA: 1:15 - loss: 5.7676 - accuracy: 0.3908 No face\n",
      "100/106 [===========================>..] - ETA: 56s - loss: 5.7579 - accuracy: 0.3893  No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 5.7430 - accuracy: 0.3873 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 5.7319 - accuracy: 0.3896 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "106/106 [==============================] - 1258s 12s/step - loss: 5.7319 - accuracy: 0.3896 - val_loss: 5.2898 - val_accuracy: 0.4791 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      " No face\n",
      " No face\n",
      "  2/106 [..............................] - ETA: 8:24 - loss: 5.2428 - accuracy: 0.3871 No face\n",
      "  5/106 [>.............................] - ETA: 13:05 - loss: 5.1606 - accuracy: 0.4395 No face\n",
      "  6/106 [>.............................] - ETA: 13:26 - loss: 5.0873 - accuracy: 0.4628 No face\n",
      " 13/106 [==>...........................] - ETA: 13:41 - loss: 4.8288 - accuracy: 0.4209 No face\n",
      " No face\n",
      " 17/106 [===>..........................] - ETA: 13:16 - loss: 4.7348 - accuracy: 0.4432 No face\n",
      " 19/106 [====>.........................] - ETA: 12:25 - loss: 4.7070 - accuracy: 0.4361 No face\n",
      " 23/106 [=====>........................] - ETA: 12:07 - loss: 4.6400 - accuracy: 0.4241 No face\n",
      " 30/106 [=======>......................] - ETA: 11:18 - loss: 4.5016 - accuracy: 0.4267 No face\n",
      " 31/106 [=======>......................] - ETA: 11:11 - loss: 4.4728 - accuracy: 0.4265 No face\n",
      " 33/106 [========>.....................] - ETA: 10:54 - loss: 4.4338 - accuracy: 0.4227 No face\n",
      " 34/106 [========>.....................] - ETA: 10:46 - loss: 4.4229 - accuracy: 0.4235 No face\n",
      " 37/106 [=========>....................] - ETA: 10:21 - loss: 4.3963 - accuracy: 0.4207 No face\n",
      " No face\n",
      " 40/106 [==========>...................] - ETA: 9:58 - loss: 4.3466 - accuracy: 0.4146  No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      " 41/106 [==========>...................] - ETA: 9:49 - loss: 4.3366 - accuracy: 0.4149 No face\n",
      " No face\n",
      " No face\n",
      " 42/106 [==========>...................] - ETA: 9:40 - loss: 4.3318 - accuracy: 0.4180 No face\n",
      " No face\n",
      " 43/106 [===========>..................] - ETA: 9:31 - loss: 4.3253 - accuracy: 0.4160 No face\n",
      " 48/106 [============>.................] - ETA: 8:50 - loss: 4.2625 - accuracy: 0.4099 No face\n",
      " No face\n",
      " 49/106 [============>.................] - ETA: 8:41 - loss: 4.2472 - accuracy: 0.4097 No face\n",
      " No face\n",
      " 50/106 [=============>................] - ETA: 8:32 - loss: 4.2398 - accuracy: 0.4095 No face\n",
      " 53/106 [==============>...............] - ETA: 8:05 - loss: 4.2246 - accuracy: 0.4065 No face\n",
      " 55/106 [==============>...............] - ETA: 7:48 - loss: 4.2085 - accuracy: 0.4085 No face\n",
      " 57/106 [===============>..............] - ETA: 7:30 - loss: 4.2024 - accuracy: 0.4069 No face\n",
      " 65/106 [=================>............] - ETA: 6:18 - loss: 4.1814 - accuracy: 0.4110 No face\n",
      " No face\n",
      " 73/106 [===================>..........] - ETA: 5:05 - loss: 4.1804 - accuracy: 0.4166 No face\n",
      " 76/106 [====================>.........] - ETA: 4:37 - loss: 4.1881 - accuracy: 0.4134 No face\n",
      " No face\n",
      " 77/106 [====================>.........] - ETA: 4:28 - loss: 4.1887 - accuracy: 0.4136 No face\n",
      " 78/106 [=====================>........] - ETA: 4:19 - loss: 4.1896 - accuracy: 0.4120 No face\n",
      " 80/106 [=====================>........] - ETA: 4:01 - loss: 4.1864 - accuracy: 0.4108 No face\n",
      " 81/106 [=====================>........] - ETA: 3:51 - loss: 4.1888 - accuracy: 0.4117 No face\n",
      " 84/106 [======================>.......] - ETA: 3:24 - loss: 4.1949 - accuracy: 0.4098 No face\n",
      " 88/106 [=======================>......] - ETA: 2:47 - loss: 4.1842 - accuracy: 0.4105 No face\n",
      " 96/106 [==========================>...] - ETA: 1:33 - loss: 4.1726 - accuracy: 0.4156 No face\n",
      " No face\n",
      " 97/106 [==========================>...] - ETA: 1:23 - loss: 4.1706 - accuracy: 0.4151 No face\n",
      "100/106 [===========================>..] - ETA: 55s - loss: 4.1649 - accuracy: 0.4131  No face\n",
      "103/106 [============================>.] - ETA: 27s - loss: 4.1467 - accuracy: 0.4121 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 4.1390 - accuracy: 0.4132 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1249s 12s/step - loss: 4.1390 - accuracy: 0.4132 - val_loss: 3.8061 - val_accuracy: 0.4779 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "  1/106 [..............................] - ETA: 0s - loss: 3.8220 - accuracy: 0.4062 No face\n",
      " No face\n",
      " No face\n",
      "  6/106 [>.............................] - ETA: 13:11 - loss: 3.6604 - accuracy: 0.4444 No face\n",
      "  8/106 [=>............................] - ETA: 13:33 - loss: 3.6642 - accuracy: 0.4008 No face\n",
      "  9/106 [=>............................] - ETA: 13:45 - loss: 3.7004 - accuracy: 0.3887 No face\n",
      " 10/106 [=>............................] - ETA: 13:47 - loss: 3.6711 - accuracy: 0.4045 No face\n",
      " 12/106 [==>...........................] - ETA: 13:45 - loss: 3.6436 - accuracy: 0.4032 No face\n",
      " 13/106 [==>...........................] - ETA: 13:38 - loss: 3.6281 - accuracy: 0.4020 No face\n",
      " No face\n",
      " 19/106 [====>.........................] - ETA: 13:06 - loss: 3.6768 - accuracy: 0.3997 No face\n",
      " 21/106 [====>.........................] - ETA: 12:53 - loss: 3.6948 - accuracy: 0.4024 No face\n",
      " 22/106 [=====>........................] - ETA: 12:43 - loss: 3.6998 - accuracy: 0.3974 No face\n",
      " 23/106 [=====>........................] - ETA: 12:34 - loss: 3.6967 - accuracy: 0.4011 No face\n",
      " 24/106 [=====>........................] - ETA: 12:25 - loss: 3.7196 - accuracy: 0.4098 No face\n",
      " 26/106 [======>.......................] - ETA: 12:11 - loss: 3.7406 - accuracy: 0.4137 No face\n",
      " No face\n",
      " 28/106 [======>.......................] - ETA: 11:55 - loss: 3.7454 - accuracy: 0.4107 No face\n",
      " 31/106 [=======>......................] - ETA: 11:30 - loss: 3.7783 - accuracy: 0.4035 No face\n",
      " 35/106 [========>.....................] - ETA: 10:55 - loss: 3.7977 - accuracy: 0.4069 No face\n",
      " No face\n",
      " No face\n",
      " 36/106 [=========>....................] - ETA: 10:47 - loss: 3.8045 - accuracy: 0.4088 No face\n",
      " 37/106 [=========>....................] - ETA: 10:38 - loss: 3.8137 - accuracy: 0.4117 No face\n",
      " No face\n",
      " 39/106 [==========>...................] - ETA: 10:20 - loss: 3.8249 - accuracy: 0.4137 No face\n",
      " 42/106 [==========>...................] - ETA: 9:52 - loss: 3.8606 - accuracy: 0.4029  No face\n",
      " 44/106 [===========>..................] - ETA: 9:34 - loss: 3.8662 - accuracy: 0.3990 No face\n",
      " 47/106 [============>.................] - ETA: 9:07 - loss: 3.8673 - accuracy: 0.3957 No face\n",
      " 55/106 [==============>...............] - ETA: 7:46 - loss: 3.8687 - accuracy: 0.3966 No face\n",
      " No face\n",
      " 58/106 [===============>..............] - ETA: 7:19 - loss: 3.8733 - accuracy: 0.4031 No face\n",
      " 59/106 [===============>..............] - ETA: 7:10 - loss: 3.8725 - accuracy: 0.4023 No face\n",
      " 64/106 [=================>............] - ETA: 6:25 - loss: 3.8821 - accuracy: 0.4013 No face\n",
      " 65/106 [=================>............] - ETA: 6:16 - loss: 3.8819 - accuracy: 0.4011 No face\n",
      " 67/106 [=================>............] - ETA: 5:58 - loss: 3.8824 - accuracy: 0.4029 No face\n",
      " 69/106 [==================>...........] - ETA: 5:40 - loss: 3.8836 - accuracy: 0.4032 No face\n",
      " 74/106 [===================>..........] - ETA: 4:55 - loss: 3.8871 - accuracy: 0.4027 No face\n",
      " No face\n",
      " No face\n",
      " 75/106 [====================>.........] - ETA: 4:46 - loss: 3.8863 - accuracy: 0.4007 No face\n",
      " No face\n",
      " 78/106 [=====================>........] - ETA: 4:18 - loss: 3.8884 - accuracy: 0.4000 No face\n",
      " 82/106 [======================>.......] - ETA: 3:42 - loss: 3.8980 - accuracy: 0.4005 No face\n",
      " 84/106 [======================>.......] - ETA: 3:23 - loss: 3.9011 - accuracy: 0.4015 No face\n",
      " 87/106 [=======================>......] - ETA: 2:56 - loss: 3.9075 - accuracy: 0.4022 No face\n",
      " 88/106 [=======================>......] - ETA: 2:46 - loss: 3.9086 - accuracy: 0.4017 No face\n",
      " No face\n",
      " No face\n",
      " 93/106 [=========================>....] - ETA: 2:00 - loss: 3.9189 - accuracy: 0.4051 No face\n",
      " 96/106 [==========================>...] - ETA: 1:32 - loss: 3.9373 - accuracy: 0.4053 No face\n",
      "101/106 [===========================>..] - ETA: 46s - loss: 3.9405 - accuracy: 0.4048 No face\n",
      "104/106 [============================>.] - ETA: 18s - loss: 3.9443 - accuracy: 0.4031 No face\n",
      "106/106 [==============================] - ETA: 0s - loss: 3.9410 - accuracy: 0.4048 No face\n",
      " No face\n",
      " No face\n",
      " No face\n",
      "106/106 [==============================] - 1247s 12s/step - loss: 3.9410 - accuracy: 0.4048 - val_loss: 4.0659 - val_accuracy: 0.4409 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      " No face\n",
      "  3/106 [..............................] - ETA: 10:45 - loss: 3.9537 - accuracy: 0.3579 No face\n",
      "  7/106 [>.............................] - ETA: 13:09 - loss: 3.8439 - accuracy: 0.3964 No face\n",
      " 10/106 [=>............................] - ETA: 13:28 - loss: 3.7140 - accuracy: 0.4227 No face\n",
      " 17/106 [===>..........................] - ETA: 13:11 - loss: 3.6641 - accuracy: 0.4204 No face\n",
      " 19/106 [====>.........................] - ETA: 12:59 - loss: 3.6395 - accuracy: 0.4229 No face\n",
      " 20/106 [====>.........................] - ETA: 12:52 - loss: 3.6408 - accuracy: 0.4211 No face\n",
      " No face\n",
      " 21/106 [====>.........................] - ETA: 12:43 - loss: 3.6281 - accuracy: 0.4172 No face\n",
      " 22/106 [=====>........................] - ETA: 12:36 - loss: 3.6147 - accuracy: 0.4173 No face\n",
      " 23/106 [=====>........................] - ETA: 12:28 - loss: 3.6023 - accuracy: 0.4105 No face\n",
      " 24/106 [=====>........................] - ETA: 12:21 - loss: 3.5886 - accuracy: 0.4122 No face\n",
      " 28/106 [======>.......................] - ETA: 11:51 - loss: 3.5510 - accuracy: 0.4197 No face\n",
      " 29/106 [=======>......................] - ETA: 11:42 - loss: 3.5381 - accuracy: 0.4208 No face\n",
      " 30/106 [=======>......................] - ETA: 11:33 - loss: 3.5452 - accuracy: 0.4186 No face\n"
     ]
    }
   ],
   "source": [
    "#fit_generator\n",
    "checkpoint = ModelCheckpoint(os.path.join(model_folder_path,\"03-5_resnet_mlp512-128_bs32_ep{epoch}_{val_accuracy:.4f}.h5\"), \n",
    "               save_best_only=True, save_weights_only=False)   #Defaults: save_freq='epoch', save_weights_only=False\n",
    "earlystop = EarlyStopping(patience=6, restore_best_weights=True)               \n",
    "reduce_lr = ReduceLROnPlateau(factor=0.5, \n",
    "                              min_lr=1e-12, \n",
    "                              monitor='val_loss', \n",
    "                              patience=3, \n",
    "                              verbose=1)\n",
    "\n",
    "\n",
    "#logs = age_model.fit_generator(\n",
    "logs = age_model.fit( \n",
    "        generator_train,\n",
    "        #steps_per_epoch=len(x_train)//BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=generator_test,\n",
    "        #validation_steps=len(x_test)//BATCH_SIZE,\n",
    "        class_weight = class_weight,    # `class_weight` is only supported for Models with a single output.\n",
    "        #callbacks=[earlystop]\n",
    "        callbacks=[checkpoint, earlystop, reduce_lr] \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uo-ObdSowPvw"
   },
   "outputs": [],
   "source": [
    "age_model.save(os.path.join(model_folder_path,'03-5_resnet_mlp512-128_bs32_save.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrS_mOX64Uv7"
   },
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(model_folder_path,'03-5_resnet_mlp512-128_bs32_save.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_zCJfVyisiK"
   },
   "outputs": [],
   "source": [
    "history = logs.history\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETe6gpnqjZzx"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.legend(['age_accuracy', 'val_age_accuracy'])\n",
    "plt.title('age_accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYa3D33Gi_Yx"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history['gender_accuracy'])\n",
    "# plt.plot(history['val_gender_accuracy'])\n",
    "# plt.legend(['gender_accuracy', 'val_gender_accuracy'])\n",
    "# plt.title('gender_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eibGTK-gjaTU"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.legend(['age_loss', 'val_age_loss'])\n",
    "plt.title('age_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3723697,
     "status": "aborted",
     "timestamp": 1603559149979,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "bMECQHoybWq0"
   },
   "outputs": [],
   "source": [
    "# plt.plot(history['gender_loss'])\n",
    "# plt.plot(history['val_gender_loss'])\n",
    "# plt.legend(['gender_loss', 'val_gender_loss'])\n",
    "# plt.title('gender_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3723694,
     "status": "aborted",
     "timestamp": 1603559149980,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "luQDTgMI4tGl"
   },
   "outputs": [],
   "source": [
    "!pip install keras2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3723694,
     "status": "aborted",
     "timestamp": 1603559149982,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "lCHxkdwCDnam"
   },
   "outputs": [],
   "source": [
    "# convert to onnx model\n",
    "import keras2onnx\n",
    "onnx_model = keras2onnx.convert_keras(age_model, age_model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3723692,
     "status": "aborted",
     "timestamp": 1603559149982,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "w-KrqZ3kEKet"
   },
   "outputs": [],
   "source": [
    "keras2onnx.save_model(onnx_model, os.path.join(model_folder_path,'23-5_resnet_mlp512-128_aug.onnx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBpYGPqZDTkT"
   },
   "outputs": [],
   "source": [
    "# 取得要預測的圖片並做預處理\n",
    "def get_preprocess_images(data_paths, y_data, batch_size=BATCH_SIZE):\n",
    "    n = len(data_paths)\n",
    "    # i = 0\n",
    "    # data_paths = data_paths\n",
    "    \n",
    "    #while i < n:    \n",
    "    x_ori, x_norm, y_age, y_gender = [], [], [], []\n",
    "    #i_batch = i\n",
    "    for idx in range(batch_size):\n",
    "        path = data_paths[idx]\n",
    "        #print(\"n:\", n, \"idx:\", i, \"cls:\", y_data[i], path)\n",
    "    \n",
    "        # 讀取圖片,切下臉的部分,並使用借來的模型的預處理方式來作預處理 \n",
    "        try:          \n",
    "            img = cv2.imread(os.path.join(img_folder_path,path))[:,:,::-1]\n",
    "        except:\n",
    "            print('imread failed:', path)\n",
    "            idx = idx + 1\n",
    "            continue                   \n",
    "        \n",
    "        faces = detect_faces(img)\n",
    "        if len(faces) == 0 or faces[0].shape[0] == 0 or faces[0].shape[1] == 0:\n",
    "            print('No face')\n",
    "            idx = idx + 1\n",
    "            continue   \n",
    "        # print(faces[0].shape)    \n",
    "        img_crop = cv2.resize(faces[0], (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "\n",
    "        # 使用借來的模型的預處理方式來作預處理\n",
    "        img_pre = preprocess_input(np.array(img_crop, dtype=float))\n",
    "\n",
    "        # 把原圖留下來\n",
    "        x_ori.append(img)\n",
    "        x_norm.append(img_pre)\n",
    "        if len(y_data) != 0:\n",
    "            # 2個輸出: age, gender\n",
    "            y_age.append(y_data[idx][0])\n",
    "            y_gender.append(y_data[idx][1])\n",
    "        \n",
    "        idx = idx + 1\n",
    "\n",
    "\n",
    "    # print(\"len(image_data)\",len(x_ori))\n",
    "    # plt.figure(figsize=(10, 40))\n",
    "    # for j,m in enumerate(x_ori):\n",
    "    #     plt.subplot(1, BATCH_SIZE, (j%BATCH_SIZE)+1)\n",
    "    #     plt.title(\"idx:{} y_data:{}\".format(i_batch+j, y_data[i_batch+j]))\n",
    "    #     plt.axis(\"off\")\n",
    "    #     plt.imshow(m)\n",
    "    # plt.show() \n",
    "\n",
    "    \n",
    "    # 2個輸出: age, gender  \n",
    "    # print(type(y_age), len(y_age), y_age[:8])\n",
    "    # print(type(y_gender), len(y_gender), y_gender[:8])\n",
    "    if len(y_data) != 0:\n",
    "        y_age_category = to_categorical(y_age, num_classes=8) \n",
    "        y_gender_category = to_categorical(y_gender, num_classes=2) \n",
    "        # 2個輸出: age, gender\n",
    "        y_category = {'age':np.array(y_age_category), 'gender':np.array(y_gender_category)}\n",
    "    else:\n",
    "        y_category = []\n",
    "\n",
    "    # print(type(np.array(x_norm)), np.array(x_norm).shape)\n",
    "    # print(type(y_category), np.array(y_age_category), np.array(y_gender_category))\n",
    "\n",
    "    return np.array(x_ori), np.array(x_norm), y_category\n",
    "    #print('while end', i, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngKRFAYgS5Zd"
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "#\n",
    "# 從保留作為evaluate用的資料,用generator產生資料 to predict\n",
    "x_ori, x_input, y_category = get_preprocess_images(x_eval, y_eval, batch_size=len(x_eval))\n",
    "\n",
    "# 取出圖片資料與正確答案\n",
    "x_eval_data, y_true_age, y_true_gender = [], [], []\n",
    "for i,x in enumerate(x_input):\n",
    "    # print(\"x_eval_data:\", len(list(x_dict['input_4'])))\n",
    "    x_eval_data.append(x)\n",
    "    # print(\"y_true_age:\", y_dict['age'].argmax(axis=-1))\n",
    "    # print(\"y_true_gender:\", y_dict['gender'].argmax(axis=-1))    \n",
    "    y_true_age.append( (list(y_category['age'])[i].argmax(axis=-1)) )\n",
    "    y_true_gender.append( (list(y_category['gender'])[i].argmax(axis=-1)) )\n",
    "\n",
    "# print(\"-------------------------\")\n",
    "print(\"x_eval_data:\", type(x_eval_data), \"np.array:\", np.array(x_eval_data).shape, x_eval[:8])\n",
    "print(\"y_true_age:\", y_true_age)\n",
    "print(\"y_true_gender:\", y_true_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0VkMu89Pn36"
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "pred = age_model.predict(np.array(x_eval_data))\n",
    "# 2個輸出: age, gender\n",
    "# pred_age = pred[0].argmax(axis=-1)      #pred[0] is predicted probabilities for age\n",
    "# pred_gender = pred[1].argmax(axis=-1)   #pred[1] is predicted probabilities for gender\n",
    "# 1個輸出: age\n",
    "pred_age = pred.argmax(axis=-1)  \n",
    "print(\"predict age:\",pred_age)\n",
    "#print(\"predict gender:\",pred_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkfU4iT97pGb"
   },
   "outputs": [],
   "source": [
    "np.unique(pred_age), np.unique(y_true_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6P7GHnZJ58DJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#print(np.array(y_true_age).shape, np.array(pred_age).shape, np.array(y_true_gender).shape, np.array(pred_gender).shape)\n",
    "print(np.array(y_true_age).shape, np.array(pred_age).shape)\n",
    "age_target_names = [str(i) for i in range(8)]\n",
    "#gender_target_names = [str(i) for i in range(2)]\n",
    "print(classification_report(np.array(y_true_age), np.array(pred_age), target_names=age_target_names))\n",
    "#print(classification_report(np.array(y_true_gender), np.array(pred_gender), target_names=gender_target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pFTzAkoWIiT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_true_age, pred_age),\n",
    "            index=[\"{}(真實)\".format(i) for i in range(8)],\n",
    "            columns=[\"{}(預測)\".format(i) for i in range(8)] \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVTIZJsXZyjn"
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(confusion_matrix(y_true_gender, pred_gender),\n",
    "#             index=[\"{}(真實)\".format(i) for i in range(2)],\n",
    "#             columns=[\"{}(預測)\".format(i) for i in range(2)] \n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3723678,
     "status": "aborted",
     "timestamp": 1603559149988,
     "user": {
      "displayName": "Kevin Chen",
      "photoUrl": "",
      "userId": "12550767145880732129"
     },
     "user_tz": -480
    },
    "id": "LdyuPoYL6Q7K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "03-5_resnet50_mlp512-128_Leaky_bs32_ReduceLR.ipynb",
   "provenance": [
    {
     "file_id": "1ClsD_Zs3NzVu6RTx69LxBDVhBjKuETse",
     "timestamp": 1601714473846
    },
    {
     "file_id": "11FVutJ-ji5oV_L3NQZYxD3pHZmyUdz6U",
     "timestamp": 1598945913717
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
